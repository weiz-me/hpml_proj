{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9742b9dc-1272-462e-b345-a1465ec8c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yes y |pip uninstall torch torchvision\n",
    "# !yes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe97718-7375-4469-96bb-1288cfb62938",
   "metadata": {},
   "source": [
    "# Fine-tune GPT cola\n",
    "\n",
    "Tutorial : https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d793c7d4-03e4-4860-a065-e8ceb2427087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification,AutoModelForSequenceClassification\n",
    "\n",
    "# Load tokenizer and GPT2 model with sequence classification head\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('PavanNeerudu/gpt2-finetuned-mrpc')\n",
    "model = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-mrpc', num_labels=2,torch_dtype=\"auto\") \n",
    "text = \"The inspector analyzed the soundness in the building.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "output = model(**encoded_input)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7fc8f2-bf8d-4b0a-b1b8-c06519772f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2ForSequenceClassification,AutoModelForSequenceClassification\n",
    "# from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "# text = \"The inspector analyzed the soundness in the building.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# output = model(**encoded_input)\n",
    "# # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed5905-a2a1-4807-8dea-daa57a7e4b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb0b44b-b076-4e00-8c99-c1baae733d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 8551\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1043\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1063\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\",\"cola\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"],truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\",\"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\",\"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e40dfcb-6c34-4958-a4c1-8bb6a4644948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e06d589-ead3-4985-ae8d-dcfa8e8441e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle = True, batch_size=8, collate_fn = data_collator\n",
    ")\n",
    "# train_dataloader = DataLoader(\n",
    "#     small_train_dataset, shuffle = True, batch_size=8, collate_fn = data_collator\n",
    "# )\n",
    "train2_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn = data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn = data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3e166eb-4637-42ec-89c5-62b45c769f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(),lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32991ee2-2b61-4347-ae95-128d2e103672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fafd4bae-fd0a-425c-9b6a-b420f7c32acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import get_scheduler\n",
    "\n",
    "# num_epochs = 3\n",
    "# num_training_steps = num_epochs * len(train_dataloader)\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=num_training_steps\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91e16e-735e-4068-a1bb-00a8b799b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# progress_bar = tqdm(range(num_training_steps))\n",
    "# device = \"cpu\"\n",
    "# model.to(device)\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in train_dataloader:\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c92b2-5e17-41ea-a5d6-213c906c8e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c58e8d1-7b54-44aa-99df-a577089408c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd19b2d-fcba-429c-a5c0-1a2c901481f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_size = param_size + buffer_size  # Total size in bytes\n",
    "    return total_size / (1024 ** 2)  # Convert to MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd14c7a8-0b82-4a66-912c-3db7dd1213df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.826540414436668,\n",
       " 'accuracy': 0.704362062916618,\n",
       " 'cpu time': 98.90361881256104,\n",
       " 'cuda time': 13.57767128944397,\n",
       " 'size': 486.7061004638672}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "\n",
    "cp = 1\n",
    "gp = 1\n",
    "metric = load(\"glue\",config_name=\"mrpc\")\n",
    "res = {}\n",
    "if cp:\n",
    "    device = \"cpu\"\n",
    "    model.eval()\n",
    "    # i=0\n",
    "    model.to(device)\n",
    "\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "    \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    cpu_time = end-start\n",
    "    met = metric.compute()\n",
    "    res[\"f1\"]=met[\"f1\"]\n",
    "    res[\"accuracy\"]=met[\"accuracy\"]\n",
    "else:\n",
    "    cpu_time = None\n",
    "    \n",
    "if gp:\n",
    "    device = \"cuda\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        encoded_input = {k:batch[k] for k in ['input_ids', 'attention_mask']}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_input)\n",
    "            \n",
    "    \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    cuda_time = end-start\n",
    "    if not cp:\n",
    "        met = metric.compute()\n",
    "        res[\"f1\"]=met[\"f1\"]\n",
    "        res[\"accuracy\"]=met[\"accuracy\"]\n",
    "else:\n",
    "    cuda_time = None\n",
    "res[f\"cpu time\"] = cpu_time\n",
    "res[f\"cuda time\"] = cuda_time\n",
    "res[f\"size\"] = get_model_size(model)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccfdaafd-8396-41f5-97c2-3fd4b75b351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"./models/gpt2_mrpc.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99a5cbc0-adee-4c09-a158-141c10367810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_mrpc.json\", \"w\") as json_file:\n",
    "    json.dump(res, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cb36d82-7a3b-4689-96e7-0bdb373d61fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad7cc93d-ac0a-46ee-9b33-bca78e382e0f",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Tutorial: https://pytorch.org/tutorials/recipes/quantization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338a714-f281-4b81-827d-fa2da36fa2ef",
   "metadata": {},
   "source": [
    "#### dynamic quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9062752d-7031-41f3-bc3f-87c5b63cd30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-mrpc', num_labels=2) \n",
    "\n",
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f5eb7a9-506d-4403-8179-acd558c959ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7029393370856786,\n",
       " 'f1': 0.8255600440690415,\n",
       " 'cpu time': 11.698429822921753}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "model_dynamic_quantized_int8.eval()\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_dynamic_quantized_int8(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf606cba-3231-4323-9152-75fbe0fb5783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7029393370856786,\n",
       " 'f1': 0.8255600440690415,\n",
       " 'cpu time': 11.698429822921753,\n",
       " 'cuda time': None,\n",
       " 'size': 486.7002410888672}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized_int8)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1ba54fe-980c-4246-83de-7262f14c8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_mrpc_dynamic_qint8.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/gpt2_int8.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406450a-1a45-4395-b451-caadc2ca1df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f23ad728-8245-4956-92ac-54ee952a5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_float16 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "424dbdf0-8513-46b8-aa7a-2ef785db2b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6912751677852349,\n",
       " 'f1': 0.8174603174603174,\n",
       " 'cpu time': 11.756733894348145}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_float16.to(device)\n",
    "\n",
    "model_dynamic_quantized_float16.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_dynamic_quantized_float16(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "195abb19-78f0-4a4d-b769-9e8695f8d16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6912751677852349,\n",
       " 'f1': 0.8174603174603174,\n",
       " 'cpu time': 11.756733894348145,\n",
       " 'cuda time': None,\n",
       " 'size': 486.7002410888672}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized_float16)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbbbcfb0-b174-4e41-a8d7-242b6f871885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_mrpc_dynamic_float16.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/gpt2_float16.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bd5ec-d416-4e09-8bd0-33659c39b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f460d31-cf6c-4145-bfed-07d62df10178",
   "metadata": {},
   "source": [
    "### Model Prunning\n",
    "Tutorial: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c64094-4fae-43ff-bc12-fbb52b2a2dea",
   "metadata": {},
   "source": [
    "##### L1-Norm Unstructure Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad401f74-ee1a-42e0-8b34-d189bfafa37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "model_prun_unstructure = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-mrpc', num_labels=2) \n",
    "\n",
    "# model.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "788be42d-0ae1-43c8-951c-09de7d19b8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1D(nf=2304, nx=768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b21a65e-1cb4-4eea-971f-fd97d438cd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2SdpaAttention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn\n",
    "model.transformer.h[0].attn.c_proj\n",
    "model.transformer.h[0].mlp.c_fc\n",
    "model.transformer.h[0].mlp.c_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ee35c0-6660-4386-ad1c-fda10ce813c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prun percent 10%\n",
      "{'accuracy': 0.6912751677852349, 'f1': 0.8174603174603174}\n",
      "prun percent 20%\n",
      "{'accuracy': 0.6912751677852349, 'f1': 0.8174603174603174}\n",
      "prun percent 30%\n",
      "{'accuracy': 0.6912751677852349, 'f1': 0.8174603174603174}\n",
      "prun percent 40%\n",
      "{'accuracy': 0.6912751677852349, 'f1': 0.8174603174603174}\n",
      "prun percent 50%\n",
      "{'accuracy': 0.6912751677852349, 'f1': 0.8174603174603174}\n",
      "prun percent 60%\n",
      "{'accuracy': 0.6807286673058485, 'f1': 0.8080691642651296}\n",
      "prun percent 70%\n",
      "{'accuracy': 0.6912751677852349, 'f1': 0.8174603174603174}\n",
      "prun percent 80%\n",
      "{'accuracy': 0.6912751677852349, 'f1': 0.8174603174603174}\n",
      "prun percent 90%\n",
      "{'accuracy': 0.6912751677852349, 'f1': 0.8174603174603174}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'percent': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
       " 'f1': [0.8174603174603174,\n",
       "  0.8174603174603174,\n",
       "  0.8174603174603174,\n",
       "  0.8174603174603174,\n",
       "  0.8174603174603174,\n",
       "  0.8080691642651296,\n",
       "  0.8174603174603174,\n",
       "  0.8174603174603174,\n",
       "  0.8174603174603174],\n",
       " 'cuda time': [2.7447926998138428,\n",
       "  3.4886314868927,\n",
       "  3.293457269668579,\n",
       "  3.4192774295806885,\n",
       "  3.3696861267089844,\n",
       "  3.312985897064209,\n",
       "  3.0935909748077393,\n",
       "  3.1403775215148926,\n",
       "  3.037839651107788],\n",
       " 'cpu time': [18.81718349456787,\n",
       "  17.8518967628479,\n",
       "  22.070685148239136,\n",
       "  17.866071224212646,\n",
       "  17.143295764923096,\n",
       "  16.89777159690857,\n",
       "  16.774181604385376,\n",
       "  15.539551973342896,\n",
       "  15.600200176239014],\n",
       " 'accuracy': [0.6912751677852349,\n",
       "  0.6912751677852349,\n",
       "  0.6912751677852349,\n",
       "  0.6912751677852349,\n",
       "  0.6912751677852349,\n",
       "  0.6807286673058485,\n",
       "  0.6912751677852349,\n",
       "  0.6912751677852349,\n",
       "  0.6912751677852349],\n",
       " 'type': ['unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "prun_data = {\"percent\":[],\"f1\":[],\"cuda time\":[],\"cpu time\":[],\"accuracy\":[],\"type\":[],}\n",
    "for i in range(1,10):\n",
    "    print(f\"prun percent {i*10}%\")\n",
    "    metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "    model_prun_unstructure = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-mrpc', num_labels=2) \n",
    "    amt = i/10\n",
    "    prune.l1_unstructured(model_prun_unstructure.transformer.wte, name=\"weight\", amount=amt)\n",
    "    prune.l1_unstructured(model_prun_unstructure.transformer.wpe, name=\"weight\", amount=amt)\n",
    "\n",
    "    for layer_idx in range(12):\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].attn.c_attn, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].attn.c_proj, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].mlp.c_fc, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].mlp.c_proj, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].attn.c_attn, name=\"bias\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].attn.c_proj, name=\"bias\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].mlp.c_fc, name=\"bias\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].mlp.c_proj, name=\"bias\", amount=amt)\n",
    "\n",
    "        prune.remove(model_prun_unstructure.transformer.h[layer_idx].attn.c_attn, name=\"weight\")\n",
    "        prune.remove(model_prun_unstructure.transformer.h[layer_idx].attn.c_proj, name=\"weight\")\n",
    "        prune.remove(model_prun_unstructure.transformer.h[layer_idx].mlp.c_fc, name=\"weight\")\n",
    "        prune.remove(model_prun_unstructure.transformer.h[layer_idx].mlp.c_proj, name=\"weight\")\n",
    "\n",
    "    prune.l1_unstructured(model_prun_unstructure.transformer.ln_f, name=\"weight\", amount=amt)\n",
    "\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    metric_res = metric.compute()\n",
    "    print(metric_res)\n",
    "    prun_data[\"cuda time\"].append(end - start)\n",
    "    prun_data[\"f1\"].append(metric_res[\"f1\"])\n",
    "    prun_data[\"accuracy\"].append(metric_res[\"accuracy\"])\n",
    "    prun_data[\"type\"].append(\"unstructure\")\n",
    "    prun_data[\"percent\"].append(i*10)\n",
    "\n",
    "\n",
    "    device = \"cpu\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    prun_data[\"cpu time\"].append(end - start)\n",
    "\n",
    "prun_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb311b6-2ce2-45f2-ac3c-6262afa5de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_mrpc_prun_unstructure.json\", \"w\") as json_file:\n",
    "    json.dump(prun_data, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342b9d1-8391-4f06-b1f6-d5db112e8e75",
   "metadata": {},
   "source": [
    "##### Prun structure \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a400836-1f05-4478-b67b-2d990dde78dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prun percent 10%\n",
      "{'accuracy': 0.6874400767018217, 'f1': 0.8145620022753128}\n",
      "prun percent 20%\n",
      "{'accuracy': 0.6558005752636625, 'f1': 0.7882005899705015}\n",
      "prun percent 30%\n",
      "{'accuracy': 0.6510067114093959, 'f1': 0.773067331670823}\n",
      "prun percent 40%\n",
      "{'accuracy': 0.3096836049856184, 'f1': 0.008264462809917356}\n",
      "prun percent 50%\n",
      "{'accuracy': 0.311601150527325, 'f1': 0.008287292817679558}\n",
      "prun percent 60%\n",
      "{'accuracy': 0.3096836049856184, 'f1': 0.002770083102493075}\n",
      "prun percent 70%\n",
      "{'accuracy': 0.3087248322147651, 'f1': 0.0}\n",
      "prun percent 80%\n",
      "{'accuracy': 0.311601150527325, 'f1': 0.008287292817679558}\n",
      "prun percent 90%\n",
      "{'accuracy': 0.3087248322147651, 'f1': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'percent': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
       " 'f1': [0.8145620022753128,\n",
       "  0.7882005899705015,\n",
       "  0.773067331670823,\n",
       "  0.008264462809917356,\n",
       "  0.008287292817679558,\n",
       "  0.002770083102493075,\n",
       "  0.0,\n",
       "  0.008287292817679558,\n",
       "  0.0],\n",
       " 'cuda time': [2.8779895305633545,\n",
       "  2.766774892807007,\n",
       "  2.8566083908081055,\n",
       "  2.7322964668273926,\n",
       "  2.8842616081237793,\n",
       "  2.8876256942749023,\n",
       "  3.004941701889038,\n",
       "  2.828578472137451,\n",
       "  2.759298324584961],\n",
       " 'cpu time': [15.589048385620117,\n",
       "  15.770520210266113,\n",
       "  15.961583614349365,\n",
       "  15.930171489715576,\n",
       "  15.857511281967163,\n",
       "  15.787281513214111,\n",
       "  15.796875,\n",
       "  17.08060312271118,\n",
       "  16.890757083892822],\n",
       " 'accuracy': [0.6874400767018217,\n",
       "  0.6558005752636625,\n",
       "  0.6510067114093959,\n",
       "  0.3096836049856184,\n",
       "  0.311601150527325,\n",
       "  0.3096836049856184,\n",
       "  0.3087248322147651,\n",
       "  0.311601150527325,\n",
       "  0.3087248322147651],\n",
       " 'type': ['ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prun_data = {\"percent\":[],\"f1\":[],\"cuda time\":[],\"cpu time\":[],\"accuracy\":[],\"type\":[],}\n",
    "for i in range(1,10):\n",
    "    print(f\"prun percent {i*10}%\")\n",
    "    metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "    model_prun_structure = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-mrpc', num_labels=2) \n",
    "    amt = i/10\n",
    "\n",
    "    # print(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.query.weight)\n",
    "    \n",
    "    prune.ln_structured(model_prun_structure.transformer.wte, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "    prune.ln_structured(model_prun_structure.transformer.wpe, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "\n",
    "    for layer_idx in range(12):\n",
    "        # Access attention layers (query, key, value)\n",
    "        prune.ln_structured(model_prun_structure.transformer.h[layer_idx].attn.c_attn, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "        prune.ln_structured(model_prun_structure.transformer.h[layer_idx].attn.c_proj, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "        prune.ln_structured(model_prun_structure.transformer.h[layer_idx].mlp.c_fc, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "        prune.ln_structured(model_prun_structure.transformer.h[layer_idx].mlp.c_proj, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "        # prune.ln_structured(model_prun_unstructure.transformer.h[layer_idx].attn.c_attn, name=\"bias\", amount=amt,n=1,dim=0)\n",
    "        # prune.ln_structured(model_prun_unstructure.transformer.h[layer_idx].attn.c_proj, name=\"bias\", amount=amt,n=1,dim=0)\n",
    "        # prune.ln_structured(model_prun_unstructure.transformer.h[layer_idx].mlp.c_fc, name=\"bias\", amount=amt,n=1,dim=0)\n",
    "        # prune.ln_structured(model_prun_unstructure.transformer.h[layer_idx].mlp.c_proj, name=\"bias\", amount=amt,n=1,dim=0)\n",
    "\n",
    "        prune.remove(model_prun_structure.transformer.h[layer_idx].attn.c_attn, name=\"weight\")\n",
    "        prune.remove(model_prun_structure.transformer.h[layer_idx].attn.c_proj, name=\"weight\")\n",
    "        prune.remove(model_prun_structure.transformer.h[layer_idx].mlp.c_fc, name=\"weight\")\n",
    "        prune.remove(model_prun_structure.transformer.h[layer_idx].mlp.c_proj, name=\"weight\")\n",
    "\n",
    "    # prune.ln_structured(model_prun_unstructure.transformer.ln_f, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    model_prun_structure.to(device)\n",
    "    \n",
    "    model_prun_structure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_structure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    metric_res = metric.compute()\n",
    "    print(metric_res)\n",
    "    prun_data[\"cuda time\"].append(end - start)\n",
    "    prun_data[\"f1\"].append(metric_res[\"f1\"])\n",
    "    prun_data[\"accuracy\"].append(metric_res[\"accuracy\"])\n",
    "    prun_data[\"type\"].append(\"ln_structure\")\n",
    "    prun_data[\"percent\"].append(i*10)\n",
    "\n",
    "\n",
    "    device = \"cpu\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    prun_data[\"cpu time\"].append(end - start)\n",
    "\n",
    "prun_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9fbfb-f8b5-4e3f-b7c4-022a5f23b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "723e959f-d00d-4adf-a613-0c0e0aa2c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_mrpc_prun_structure.json\", \"w\") as json_file:\n",
    "    json.dump(prun_data, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7886e3f-7b75-40ea-9331-f2972fb8e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prun_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19487b-6e2f-41cd-aa86-57b748fd4f2d",
   "metadata": {},
   "source": [
    "### Flash Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ecc2f97-32b3-48d1-9716-04a04693326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6912751677852349,\n",
       " 'f1': 0.8174603174603174,\n",
       " 'cpu time': 13.1011962890625,\n",
       " 'cuda time': 2.553781032562256}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sdpa = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-mrpc', num_labels=2,attn_implementation=\"sdpa\") \n",
    "\n",
    "device = \"cpu\"\n",
    "model_sdpa.to(device)\n",
    "\n",
    "model_sdpa.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_sdpa(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "model_sdpa.to(device)\n",
    "\n",
    "model_sdpa.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_sdpa(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "\n",
    "res2[\"cuda time\"] = end - start\n",
    "\n",
    "res2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5bf8f24-1eb8-4f89-9904-a696fdd35d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6912751677852349,\n",
       " 'f1': 0.8174603174603174,\n",
       " 'cpu time': 13.1011962890625,\n",
       " 'cuda time': 2.553781032562256,\n",
       " 'size': 486.7061004638672}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_sdpa)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2\n",
    "\n",
    "# device = \"cuda\"\n",
    "# model_flash_attention.to(device)\n",
    "\n",
    "# model_flash_attention.eval()\n",
    "# start = time.time()\n",
    "# for batch in eval_dataloader:\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     # with torch.no_grad():\n",
    "#     with torch.inference_mode():\n",
    "#         # raise error if no optimized kernel is available\n",
    "#         with torch.backends.cuda.sdp_kernel(\n",
    "#             enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "#         ):\n",
    "#             outputs = model_flash_attention(**batch)\n",
    "#         # print(outputs)\n",
    "#         # break\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "# end = time.time()\n",
    "# # res2 = metric.compute()\n",
    "# res2[\"cuda time\"] = end - start\n",
    "# res2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05afd9e7-720d-49cb-920c-9de7f837ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_mrpc_sdpa.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_sdpa.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_sdpa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e5904-1198-43b2-bdd8-f10b56bd9a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f371559-c376-465b-b689-afcdedf6ba56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ee36d-4d84-4f2c-b06e-b85e0d2c41c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d230898b-ddc9-4a67-aeee-badee2c63bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6912751677852349,\n",
       " 'f1': 0.8174603174603174,\n",
       " 'cpu time': 11.96101689338684,\n",
       " 'cuda time': 2.392256498336792}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eager = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-mrpc', num_labels=2,attn_implementation=\"sdpa\") \n",
    "\n",
    "device = \"cpu\"\n",
    "model_eager.to(device)\n",
    "\n",
    "model_eager.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_eager(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "model_eager.to(device)\n",
    "\n",
    "model_eager.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_eager(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2[\"cuda time\"] = end - start\n",
    "\n",
    "\n",
    "res2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce9d13e0-5ce4-4290-93d2-9d490a3cb63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6912751677852349,\n",
       " 'f1': 0.8174603174603174,\n",
       " 'cpu time': 11.96101689338684,\n",
       " 'cuda time': 2.392256498336792,\n",
       " 'size': 486.7061004638672}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_eager)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2\n",
    "\n",
    "# device = \"cuda\"\n",
    "# model_eager.to(device)\n",
    "\n",
    "# model_eager.eval()\n",
    "# start = time.time()\n",
    "# for batch in eval_dataloader:\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     # with torch.no_grad():\n",
    "#     with torch.inference_mode():\n",
    "#         # raise error if no optimized kernel is available\n",
    "#         with torch.backends.cuda.sdp_kernel(\n",
    "#             enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "#         ):\n",
    "#             outputs = model_eager(**batch)\n",
    "#         # print(outputs)\n",
    "#         # break\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "# end = time.time()\n",
    "# # res2 = metric.compute()\n",
    "# res2[\"cuda time\"] = end - start\n",
    "# res2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2da6cebc-1c28-4d62-9a5c-d0643fe0af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_mrpc_eager.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_eager.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_eager, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ce3a6-a261-4b81-956d-e228cc57be69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "442d3b8f-4054-4562-93fb-aff9a513ccae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6912751677852349,\n",
       " 'f1': 0.8174603174603174,\n",
       " 'cpu time': 11.96101689338684,\n",
       " 'cuda time': 2.8964767456054688,\n",
       " 'size': 486.7061004638672}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_flash = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-mrpc', num_labels=2,attn_implementation=\"flash_attention_2\") \n",
    "model_flash.half()\n",
    "# device = \"cpu\"\n",
    "# model_flash.to(device)\n",
    "\n",
    "# model_flash.eval()\n",
    "# start = time.time()\n",
    "# for batch in eval_dataloader:\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     # with torch.no_grad():\n",
    "#     with torch.inference_mode():\n",
    "#         # raise error if no optimized kernel is available\n",
    "#         with torch.backends.cuda.sdp_kernel(\n",
    "#             enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "#         ):\n",
    "#             outputs = model_flash(**batch)\n",
    "#         # print(outputs)\n",
    "#         # break\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "# end = time.time()\n",
    "# res2 = metric.compute()\n",
    "# res2[\"cpu time\"] = end - start\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "model_flash.to(device)\n",
    "\n",
    "model_flash.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_flash(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2[\"cuda time\"] = end - start\n",
    "\n",
    "\n",
    "res2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a53c82d7-b414-4fe4-bd0b-4dd57713485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_mrpc_flash.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204fbb3c-f6ff-4768-94f0-32164f644d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5c739-8cb8-46c7-b085-5e39ee4d20e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
