{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9742b9dc-1272-462e-b345-a1465ec8c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yes y |pip uninstall torch torchvision\n",
    "# !yes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe97718-7375-4469-96bb-1288cfb62938",
   "metadata": {},
   "source": [
    "# Fine-tune Bert cola\n",
    "\n",
    "Tutorial : https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217206aa-ac08-40e2-ab63-1544106dbde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c2ec96068e46b0bf9758ebf3162799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f128364822b548a1b5e331d2cd7ad092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c1c76ae9414fd4a8febce7d4d0d94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ffd1d7213c44f79e97d7cebde33e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6753d188563a4958a7eb6847f42a577e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel,BertForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained('pmthangk09/bert-base-uncased-glue-cola')\n",
    "model = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-cola\")\n",
    "text = \"The inspector analyzed the soundness in the building.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb0b44b-b076-4e00-8c99-c1baae733d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ef2616371647e1a42b8ede39733734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bbe43d6e0a4744b9658aa01245f4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab1f217235c476b8847cb1286cb3d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 8551\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1043\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1063\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\",\"cola\")\n",
    "tokenizer = BertTokenizer.from_pretrained('pmthangk09/bert-base-uncased-glue-cola')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\",\"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\",\"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e06d589-ead3-4985-ae8d-dcfa8e8441e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle = True, batch_size=8, collate_fn = data_collator\n",
    ")\n",
    "train2_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn = data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn = data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e166eb-4637-42ec-89c5-62b45c769f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(),lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32991ee2-2b61-4347-ae95-128d2e103672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd19b2d-fcba-429c-a5c0-1a2c901481f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd14c7a8-0b82-4a66-912c-3db7dd1213df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9e9559946c4cac8f639c9d378dec25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8283796740172579,\n",
       " 'f1': 0.8844415752098128,\n",
       " 'cuda time': 1.8768351078033447}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # encode_input = {}\n",
    "\n",
    "    # for i in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "    #     encode_input[i] = batch[i].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res = metric.compute()\n",
    "res[f\"{device} time\"] = end-start\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a5cbc0-adee-4c09-a158-141c10367810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8283796740172579,\n",
       " 'f1': 0.8844415752098128,\n",
       " 'cuda time': 1.8768351078033447,\n",
       " 'cpu time': 11.699313879013062}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res[f\"cpu time\"] = end-start\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cb36d82-7a3b-4689-96e7-0bdb373d61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"./models/bert_cola.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ad108-ba53-4c00-9f65-65f5cdfe0bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca40889e-a67d-445c-84b0-3fd80d4ef375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8283796740172579,\n",
       " 'f1': 0.8844415752098128,\n",
       " 'cuda time': 1.8768351078033447,\n",
       " 'cpu time': 11.699313879013062,\n",
       " 'size': 417.65528106689453}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "def get_model_size(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_size = param_size + buffer_size  # Total size in bytes\n",
    "    return total_size / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "size_in_mb = get_model_size(model)\n",
    "res[\"size\"] = size_in_mb\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc8de308-2c3d-4ed1-aab1-378f92018dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_cola.json\", \"w\") as json_file:\n",
    "    json.dump(res, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cc93d-ac0a-46ee-9b33-bca78e382e0f",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Tutorial: https://pytorch.org/tutorials/recipes/quantization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338a714-f281-4b81-827d-fa2da36fa2ef",
   "metadata": {},
   "source": [
    "#### dynamic quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9062752d-7031-41f3-bc3f-87c5b63cd30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-cola\")\n",
    "\n",
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f5eb7a9-506d-4403-8179-acd558c959ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.824065196548418,\n",
       " 'f1': 0.881498224087827,\n",
       " 'cpu time': 7.190145492553711}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "model_dynamic_quantized_int8.eval()\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_dynamic_quantized_int8(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf606cba-3231-4323-9152-75fbe0fb5783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.824065196548418,\n",
       " 'f1': 0.881498224087827,\n",
       " 'cpu time': 7.190145492553711,\n",
       " 'cuda time': None,\n",
       " 'size': 91.080078125}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized_int8)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1ba54fe-980c-4246-83de-7262f14c8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_cola_dynamic_qint8.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_cola_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_int8.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406450a-1a45-4395-b451-caadc2ca1df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f23ad728-8245-4956-92ac-54ee952a5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cpu\"\n",
    "model_dynamic_quantized_float16 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "424dbdf0-8513-46b8-aa7a-2ef785db2b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8283796740172579,\n",
       " 'f1': 0.8844415752098128,\n",
       " 'cpu time': 11.00797176361084}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_float16.to(device)\n",
    "\n",
    "model_dynamic_quantized_float16.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_dynamic_quantized_float16(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "195abb19-78f0-4a4d-b769-9e8695f8d16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8283796740172579,\n",
       " 'f1': 0.8844415752098128,\n",
       " 'cpu time': 11.00797176361084,\n",
       " 'cuda time': None,\n",
       " 'size': 91.080078125}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized_float16)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbbbcfb0-b174-4e41-a8d7-242b6f871885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_cola_dynamic_float16.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_cola_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_float16.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bd5ec-d416-4e09-8bd0-33659c39b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f460d31-cf6c-4145-bfed-07d62df10178",
   "metadata": {},
   "source": [
    "### Model Prunning\n",
    "Tutorial: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c64094-4fae-43ff-bc12-fbb52b2a2dea",
   "metadata": {},
   "source": [
    "##### L1-Norm Unstructure Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad401f74-ee1a-42e0-8b34-d189bfafa37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "model_prun_unstructure = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-cola\")\n",
    "# model.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6ee35c0-6660-4386-ad1c-fda10ce813c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prun percent 10%\n",
      "{'accuracy': 0.8293384467881112, 'f1': 0.8853092783505154}\n",
      "prun percent 20%\n",
      "{'accuracy': 0.822627037392138, 'f1': 0.880722114764668}\n",
      "prun percent 30%\n",
      "{'accuracy': 0.8293384467881112, 'f1': 0.8850129198966409}\n",
      "prun percent 40%\n",
      "{'accuracy': 0.8216682646212847, 'f1': 0.8785900783289817}\n",
      "prun percent 50%\n",
      "{'accuracy': 0.7267497603068073, 'f1': 0.7806004618937644}\n",
      "prun percent 60%\n",
      "{'accuracy': 0.436241610738255, 'f1': 0.3495575221238938}\n",
      "prun percent 70%\n",
      "{'accuracy': 0.3700862895493768, 'f1': 0.2351571594877765}\n",
      "prun percent 80%\n",
      "{'accuracy': 0.40651965484180247, 'f1': 0.3913470993117011}\n",
      "prun percent 90%\n",
      "{'accuracy': 0.6883988494726749, 'f1': 0.8154457694491766}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'percent': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
       " 'f1': [0.8853092783505154,\n",
       "  0.880722114764668,\n",
       "  0.8850129198966409,\n",
       "  0.8785900783289817,\n",
       "  0.7806004618937644,\n",
       "  0.3495575221238938,\n",
       "  0.2351571594877765,\n",
       "  0.3913470993117011,\n",
       "  0.8154457694491766],\n",
       " 'cuda time': [1.6929144859313965,\n",
       "  2.6742303371429443,\n",
       "  2.776714324951172,\n",
       "  2.7129549980163574,\n",
       "  2.696964740753174,\n",
       "  2.504974842071533,\n",
       "  2.6727609634399414,\n",
       "  2.817477226257324,\n",
       "  3.0889394283294678],\n",
       " 'cpu time': [11.906315803527832,\n",
       "  13.240327596664429,\n",
       "  12.857508897781372,\n",
       "  13.130394220352173,\n",
       "  13.263718605041504,\n",
       "  12.832561731338501,\n",
       "  13.317347049713135,\n",
       "  13.237804889678955,\n",
       "  18.374518156051636],\n",
       " 'accuracy': [0.8293384467881112,\n",
       "  0.822627037392138,\n",
       "  0.8293384467881112,\n",
       "  0.8216682646212847,\n",
       "  0.7267497603068073,\n",
       "  0.436241610738255,\n",
       "  0.3700862895493768,\n",
       "  0.40651965484180247,\n",
       "  0.6883988494726749],\n",
       " 'type': ['unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "\n",
    "prun_data = {\"percent\":[],\"f1\":[],\"cuda time\":[],\"cpu time\":[],\"accuracy\":[],\"type\":[],}\n",
    "for i in range(1,10):\n",
    "    print(f\"prun percent {i*10}%\")\n",
    "    metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "    model_prun_unstructure = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-cola\")\n",
    "\n",
    "    for layer_idx in range(12):\n",
    "        # Access attention layers (query, key, value)\n",
    "        amt = i/10\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.query, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.key, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.value, name=\"weight\", amount=amt)\n",
    "        \n",
    "        # Access feed-forward layers (intermediate dense layer)\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].intermediate.dense, name=\"weight\", amount=amt)\n",
    "        \n",
    "        # Optionally, prune the output dense layer (if desired)\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].output.dense, name=\"weight\", amount=amt)\n",
    "    \n",
    "    \n",
    "        # Access attention layers (query, key, value)\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.query, name=\"weight\")\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.key, name=\"weight\")\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.value, name=\"weight\")\n",
    "        \n",
    "        # Access feed-forward layers (intermediate dense layer)\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].intermediate.dense, name=\"weight\")\n",
    "        \n",
    "        # Optionally, prune the output dense layer (if desired)\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].output.dense, name=\"weight\")\n",
    "\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    metric_res = metric.compute()\n",
    "    print(metric_res)\n",
    "    prun_data[\"cuda time\"].append(end - start)\n",
    "    prun_data[\"f1\"].append(metric_res[\"f1\"])\n",
    "    prun_data[\"accuracy\"].append(metric_res[\"accuracy\"])\n",
    "    prun_data[\"type\"].append(\"unstructure\")\n",
    "    prun_data[\"percent\"].append(i*10)\n",
    "\n",
    "\n",
    "    device = \"cpu\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    prun_data[\"cpu time\"].append(end - start)\n",
    "\n",
    "prun_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bb311b6-2ce2-45f2-ac3c-6262afa5de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_cola_prun_unstructure.json\", \"w\") as json_file:\n",
    "    json.dump(prun_data, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_cola_dynamic_qint8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342b9d1-8391-4f06-b1f6-d5db112e8e75",
   "metadata": {},
   "source": [
    "##### Prun structure \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a400836-1f05-4478-b67b-2d990dde78dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prun percent 10%\n",
      "{'accuracy': 0.6903163950143816, 'f1': 0.8167895632444696}\n",
      "prun percent 20%\n",
      "{'accuracy': 0.6653883029721956, 'f1': 0.7967384973791497}\n",
      "prun percent 30%\n",
      "{'accuracy': 0.6903163950143816, 'f1': 0.816372939169983}\n",
      "prun percent 40%\n",
      "{'accuracy': 0.6912751677852349, 'f1': 0.8174603174603174}\n",
      "prun percent 50%\n",
      "{'accuracy': 0.6912751677852349, 'f1': 0.8174603174603174}\n",
      "prun percent 60%\n",
      "{'accuracy': 0.6903163950143816, 'f1': 0.8167895632444696}\n",
      "prun percent 70%\n",
      "{'accuracy': 0.6912751677852349, 'f1': 0.8174603174603174}\n",
      "prun percent 80%\n",
      "{'accuracy': 0.6912751677852349, 'f1': 0.8174603174603174}\n",
      "prun percent 90%\n",
      "{'accuracy': 0.6912751677852349, 'f1': 0.8174603174603174}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'percent': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
       " 'f1': [0.8167895632444696,\n",
       "  0.7967384973791497,\n",
       "  0.816372939169983,\n",
       "  0.8174603174603174,\n",
       "  0.8174603174603174,\n",
       "  0.8167895632444696,\n",
       "  0.8174603174603174,\n",
       "  0.8174603174603174,\n",
       "  0.8174603174603174],\n",
       " 'cuda time': [4.134546518325806,\n",
       "  2.836993455886841,\n",
       "  2.8719277381896973,\n",
       "  2.8896429538726807,\n",
       "  2.6702089309692383,\n",
       "  4.033512353897095,\n",
       "  2.570136070251465,\n",
       "  2.840125560760498,\n",
       "  2.6193416118621826],\n",
       " 'cpu time': [16.202006340026855,\n",
       "  17.76843285560608,\n",
       "  15.903268575668335,\n",
       "  15.23781418800354,\n",
       "  14.755957841873169,\n",
       "  15.233280420303345,\n",
       "  13.369174242019653,\n",
       "  13.28218388557434,\n",
       "  13.610204935073853],\n",
       " 'accuracy': [0.6903163950143816,\n",
       "  0.6653883029721956,\n",
       "  0.6903163950143816,\n",
       "  0.6912751677852349,\n",
       "  0.6912751677852349,\n",
       "  0.6903163950143816,\n",
       "  0.6912751677852349,\n",
       "  0.6912751677852349,\n",
       "  0.6912751677852349],\n",
       " 'type': ['ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prun_data = {\"percent\":[],\"f1\":[],\"cuda time\":[],\"cpu time\":[],\"accuracy\":[],\"type\":[],}\n",
    "for i in range(1,10):\n",
    "    print(f\"prun percent {i*10}%\")\n",
    "    metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "    model_prun_structure = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-cola\")\n",
    "    amt = i/10\n",
    "\n",
    "    for layer_idx in range(12):\n",
    "        # Access attention layers (query, key, value)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.query, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.key, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.value, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "        \n",
    "        # Access feed-forward layers (intermediate dense layer)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].intermediate.dense, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "        \n",
    "        # Optionally, prune the output dense layer (if desired)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].output.dense, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "    \n",
    "    \n",
    "        # Access attention layers (query, key, value)\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.query, name=\"weight\")\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.key, name=\"weight\")\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.value, name=\"weight\")\n",
    "        \n",
    "        # Access feed-forward layers (intermediate dense layer)\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].intermediate.dense, name=\"weight\")\n",
    "        \n",
    "        # Optionally, prune the output dense layer (if desired)\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].output.dense, name=\"weight\")\n",
    "\n",
    "    # print(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.query.weight)\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    model_prun_structure.to(device)\n",
    "    \n",
    "    model_prun_structure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_structure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    metric_res = metric.compute()\n",
    "    print(metric_res)\n",
    "    prun_data[\"cuda time\"].append(end - start)\n",
    "    prun_data[\"f1\"].append(metric_res[\"f1\"])\n",
    "    prun_data[\"accuracy\"].append(metric_res[\"accuracy\"])\n",
    "    prun_data[\"type\"].append(\"ln_structure\")\n",
    "    prun_data[\"percent\"].append(i*10)\n",
    "\n",
    "\n",
    "    device = \"cpu\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    prun_data[\"cpu time\"].append(end - start)\n",
    "\n",
    "prun_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de9fbfb-f8b5-4e3f-b7c4-022a5f23b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prun_data = {'percent': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
    "#  'f1': [0.8167895632444696,\n",
    "#   0.7967384973791497,\n",
    "#   0.816372939169983,\n",
    "#   0.8174603174603174,\n",
    "#   0.8174603174603174,\n",
    "#   0.8167895632444696,\n",
    "#   0.8174603174603174,\n",
    "#   0.8174603174603174,\n",
    "#   0.8174603174603174],\n",
    "#  'cuda time': [4.134546518325806,\n",
    "#   2.836993455886841,\n",
    "#   2.8719277381896973,\n",
    "#   2.8896429538726807,\n",
    "#   2.6702089309692383,\n",
    "#   4.033512353897095,\n",
    "#   2.570136070251465,\n",
    "#   2.840125560760498,\n",
    "#   2.6193416118621826],\n",
    "#  'cpu time': [16.202006340026855,\n",
    "#   17.76843285560608,\n",
    "#   15.903268575668335,\n",
    "#   15.23781418800354,\n",
    "#   14.755957841873169,\n",
    "#   15.233280420303345,\n",
    "#   13.369174242019653,\n",
    "#   13.28218388557434,\n",
    "#   13.610204935073853],\n",
    "#  'accuracy': [0.6903163950143816,\n",
    "#   0.6653883029721956,\n",
    "#   0.6903163950143816,\n",
    "#   0.6912751677852349,\n",
    "#   0.6912751677852349,\n",
    "#   0.6903163950143816,\n",
    "#   0.6912751677852349,\n",
    "#   0.6912751677852349,\n",
    "#   0.6912751677852349],\n",
    "#  'type': ['ln_structure',\n",
    "#   'ln_structure',\n",
    "#   'ln_structure',\n",
    "#   'ln_structure',\n",
    "#   'ln_structure',\n",
    "#   'ln_structure',\n",
    "#   'ln_structure',\n",
    "#   'ln_structure',\n",
    "#   'ln_structure']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "723e959f-d00d-4adf-a613-0c0e0aa2c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_cola_prun_structure.json\", \"w\") as json_file:\n",
    "    json.dump(prun_data, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_cola_dynamic_qint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7886e3f-7b75-40ea-9331-f2972fb8e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prun_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19487b-6e2f-41cd-aa86-57b748fd4f2d",
   "metadata": {},
   "source": [
    "### Flash Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ecc2f97-32b3-48d1-9716-04a04693326f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8283796740172579,\n",
       " 'f1': 0.8844415752098128,\n",
       " 'cpu time': 13.959874153137207}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from evaluate import load\n",
    "\n",
    "model_sdpa = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-cola\" ,attn_implementation=\"sdpa\")\n",
    "metric = load(\"glue\",\"mrpc\")\n",
    "\n",
    "device = \"cpu\"\n",
    "model_sdpa.to(device)\n",
    "\n",
    "model_sdpa.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_sdpa(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5bf8f24-1eb8-4f89-9904-a696fdd35d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8283796740172579,\n",
       " 'f1': 0.8844415752098128,\n",
       " 'cpu time': 13.959874153137207,\n",
       " 'cuda time': 2.6849753856658936,\n",
       " 'size': 417.65528106689453}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\"\n",
    "metric = load(\"glue\",\"cola\")\n",
    "\n",
    "model_sdpa.to(device)\n",
    "\n",
    "model_sdpa.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_sdpa(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "# res2 = metric.compute()\n",
    "res2[\"cuda time\"] = end - start\n",
    "res2\n",
    "size_in_mb = get_model_size(model_sdpa)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05afd9e7-720d-49cb-920c-9de7f837ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "with open(\"results/bert_cola_sdpa.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_cola_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_sdpa.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_sdpa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e5904-1198-43b2-bdd8-f10b56bd9a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f371559-c376-465b-b689-afcdedf6ba56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ee36d-4d84-4f2c-b06e-b85e0d2c41c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d230898b-ddc9-4a67-aeee-badee2c63bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8283796740172579,\n",
       " 'f1': 0.8844415752098128,\n",
       " 'cpu time': 14.042388916015625}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eager = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-cola\" ,attn_implementation=\"eager\")\n",
    "\n",
    "device = \"cpu\"\n",
    "model_eager.to(device)\n",
    "metric = load(\"glue\",\"mrpc\")\n",
    "\n",
    "model_eager.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_eager(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce9d13e0-5ce4-4290-93d2-9d490a3cb63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8283796740172579,\n",
       " 'f1': 0.8844415752098128,\n",
       " 'cpu time': 14.042388916015625,\n",
       " 'cuda time': 2.837186574935913}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\"\n",
    "model_eager.to(device)\n",
    "metric = load(\"glue\",\"cola\")\n",
    "\n",
    "model_eager.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_eager(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "# res2 = metric.compute()\n",
    "res2[\"cuda time\"] = end - start\n",
    "res2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2da6cebc-1c28-4d62-9a5c-d0643fe0af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_cola_eager.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_cola_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_eager.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_eager, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ce3a6-a261-4b81-956d-e228cc57be69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
