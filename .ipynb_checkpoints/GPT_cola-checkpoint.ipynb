{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9742b9dc-1272-462e-b345-a1465ec8c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yes y |pip uninstall torch torchvision\n",
    "# !yes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe97718-7375-4469-96bb-1288cfb62938",
   "metadata": {},
   "source": [
    "# Fine-tune GPT cola\n",
    "\n",
    "Tutorial : https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d793c7d4-03e4-4860-a065-e8ceb2427087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19ee1ed136e439c945614382ac3e1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ca084447ed49b484e0df31e384594a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11881c4470094789980072c652610d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d176c0296b9c435e9ca0e771cad28678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/470 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df5ded3e7df44ba88a4b5955083f68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af0aeaef81142af826b51e42ec03a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification,AutoModelForSequenceClassification\n",
    "\n",
    "# Load tokenizer and GPT2 model with sequence classification head\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('PavanNeerudu/gpt2-finetuned-cola')\n",
    "model = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-cola', num_labels=2,torch_dtype=\"auto\") \n",
    "text = \"The inspector analyzed the soundness in the building.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "output = model(**encoded_input)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7fc8f2-bf8d-4b0a-b1b8-c06519772f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2ForSequenceClassification,AutoModelForSequenceClassification\n",
    "# from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "# text = \"The inspector analyzed the soundness in the building.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# output = model(**encoded_input)\n",
    "# # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed5905-a2a1-4807-8dea-daa57a7e4b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb0b44b-b076-4e00-8c99-c1baae733d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63ef3c2f15f4a1e8cdc21c620c9ea08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f5a68949b14a3bb2af2216596fe59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7d7efafb4a4fa49f707c6276008111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a992ec24bb44f3b92084afdf5d79b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 8551\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1043\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1063\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\",\"cola\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"],truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\",\"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\",\"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e40dfcb-6c34-4958-a4c1-8bb6a4644948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e06d589-ead3-4985-ae8d-dcfa8e8441e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle = True, batch_size=8, collate_fn = data_collator\n",
    ")\n",
    "# train_dataloader = DataLoader(\n",
    "#     small_train_dataset, shuffle = True, batch_size=8, collate_fn = data_collator\n",
    "# )\n",
    "train2_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn = data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn = data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3e166eb-4637-42ec-89c5-62b45c769f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(),lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32991ee2-2b61-4347-ae95-128d2e103672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafd4bae-fd0a-425c-9b6a-b420f7c32acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import get_scheduler\n",
    "\n",
    "# num_epochs = 3\n",
    "# num_training_steps = num_epochs * len(train_dataloader)\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=num_training_steps\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d91e16e-735e-4068-a1bb-00a8b799b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# progress_bar = tqdm(range(num_training_steps))\n",
    "# device = \"cpu\"\n",
    "# model.to(device)\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in train_dataloader:\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c92b2-5e17-41ea-a5d6-213c906c8e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c58e8d1-7b54-44aa-99df-a577089408c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd19b2d-fcba-429c-a5c0-1a2c901481f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_size = param_size + buffer_size  # Total size in bytes\n",
    "    return total_size / (1024 ** 2)  # Convert to MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd14c7a8-0b82-4a66-912c-3db7dd1213df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8555417185554172,\n",
       " 'accuracy': 0.7775647171620326,\n",
       " 'cpu time': 12.731175661087036,\n",
       " 'cuda time': 3.880117177963257,\n",
       " 'size': 486.7061004638672}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "\n",
    "cp = 1\n",
    "gp = 1\n",
    "metric = load(\"glue\",config_name=\"mrpc\")\n",
    "res = {}\n",
    "if cp:\n",
    "    device = \"cpu\"\n",
    "    model.eval()\n",
    "    # i=0\n",
    "    model.to(device)\n",
    "\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "    \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    cpu_time = end-start\n",
    "    met = metric.compute()\n",
    "    res[\"f1\"]=met[\"f1\"]\n",
    "    res[\"accuracy\"]=met[\"accuracy\"]\n",
    "else:\n",
    "    cpu_time = None\n",
    "    \n",
    "if gp:\n",
    "    device = \"cuda\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        encoded_input = {k:batch[k] for k in ['input_ids', 'attention_mask']}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_input)\n",
    "            \n",
    "    \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    cuda_time = end-start\n",
    "    if not cp:\n",
    "        met = metric.compute()\n",
    "        res[\"f1\"]=met[\"f1\"]\n",
    "        res[\"accuracy\"]=met[\"accuracy\"]\n",
    "else:\n",
    "    cuda_time = None\n",
    "res[f\"cpu time\"] = cpu_time\n",
    "res[f\"cuda time\"] = cuda_time\n",
    "res[f\"size\"] = get_model_size(model)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f400fc8b-2df3-4164-a8f6-b58a58a57d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matthews_correlation': np.float64(0.43544209947737617)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccfdaafd-8396-41f5-97c2-3fd4b75b351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"./models/gpt2_cola.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99a5cbc0-adee-4c09-a158-141c10367810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_cola.json\", \"w\") as json_file:\n",
    "    json.dump(res, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb36d82-7a3b-4689-96e7-0bdb373d61fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad7cc93d-ac0a-46ee-9b33-bca78e382e0f",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Tutorial: https://pytorch.org/tutorials/recipes/quantization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338a714-f281-4b81-827d-fa2da36fa2ef",
   "metadata": {},
   "source": [
    "#### dynamic quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9062752d-7031-41f3-bc3f-87c5b63cd30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-cola', num_labels=2) \n",
    "\n",
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f5eb7a9-506d-4403-8179-acd558c959ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7746883988494727,\n",
       " 'f1': 0.848093083387201,\n",
       " 'cpu time': 12.549953937530518}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "model_dynamic_quantized_int8.eval()\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_dynamic_quantized_int8(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf606cba-3231-4323-9152-75fbe0fb5783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7746883988494727,\n",
       " 'f1': 0.848093083387201,\n",
       " 'cpu time': 12.549953937530518,\n",
       " 'cuda time': None,\n",
       " 'size': 486.7002410888672}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized_int8)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1ba54fe-980c-4246-83de-7262f14c8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_cola_dynamic_qint8.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_cola_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/gpt2_int8.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406450a-1a45-4395-b451-caadc2ca1df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f23ad728-8245-4956-92ac-54ee952a5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_float16 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "424dbdf0-8513-46b8-aa7a-2ef785db2b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7775647171620326,\n",
       " 'f1': 0.8555417185554172,\n",
       " 'cpu time': 12.764209270477295}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_float16.to(device)\n",
    "\n",
    "model_dynamic_quantized_float16.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_dynamic_quantized_float16(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "195abb19-78f0-4a4d-b769-9e8695f8d16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7775647171620326,\n",
       " 'f1': 0.8555417185554172,\n",
       " 'cpu time': 12.764209270477295,\n",
       " 'cuda time': None,\n",
       " 'size': 486.7002410888672}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized_float16)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbbbcfb0-b174-4e41-a8d7-242b6f871885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_cola_dynamic_float16.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_cola_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/gpt2_float16.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bd5ec-d416-4e09-8bd0-33659c39b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f460d31-cf6c-4145-bfed-07d62df10178",
   "metadata": {},
   "source": [
    "### Model Prunning\n",
    "Tutorial: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c64094-4fae-43ff-bc12-fbb52b2a2dea",
   "metadata": {},
   "source": [
    "##### L1-Norm Unstructure Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad401f74-ee1a-42e0-8b34-d189bfafa37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "model_prun_unstructure = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-cola', num_labels=2) \n",
    "\n",
    "# model.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "788be42d-0ae1-43c8-951c-09de7d19b8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1D(nf=2304, nx=768)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b21a65e-1cb4-4eea-971f-fd97d438cd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1D(nf=768, nx=3072)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn\n",
    "model.transformer.h[0].attn.c_proj\n",
    "model.transformer.h[0].mlp.c_fc\n",
    "model.transformer.h[0].mlp.c_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6ee35c0-6660-4386-ad1c-fda10ce813c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prun percent 10%\n",
      "{'accuracy': 0.7727708533077661, 'f1': 0.8493324856961221}\n",
      "prun percent 20%\n",
      "{'accuracy': 0.7737296260786194, 'f1': 0.8451443569553806}\n",
      "prun percent 30%\n",
      "{'accuracy': 0.716203259827421, 'f1': 0.7764350453172205}\n",
      "prun percent 40%\n",
      "{'accuracy': 0.34132310642377756, 'f1': 0.10894941634241245}\n",
      "prun percent 50%\n",
      "{'accuracy': 0.3087248322147651, 'f1': 0.0027662517289073307}\n",
      "prun percent 60%\n",
      "{'accuracy': 0.3288590604026846, 'f1': 0.08376963350785341}\n",
      "prun percent 70%\n",
      "{'accuracy': 0.42665388302972196, 'f1': 0.4369114877589454}\n",
      "prun percent 80%\n",
      "{'accuracy': 0.311601150527325, 'f1': 0.008287292817679558}\n",
      "prun percent 90%\n",
      "{'accuracy': 0.3087248322147651, 'f1': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'percent': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
       " 'f1': [0.8493324856961221,\n",
       "  0.8451443569553806,\n",
       "  0.7764350453172205,\n",
       "  0.10894941634241245,\n",
       "  0.0027662517289073307,\n",
       "  0.08376963350785341,\n",
       "  0.4369114877589454,\n",
       "  0.008287292817679558,\n",
       "  0.0],\n",
       " 'cuda time': [2.058180809020996,\n",
       "  1.9286153316497803,\n",
       "  3.3593568801879883,\n",
       "  3.1907591819763184,\n",
       "  3.2097573280334473,\n",
       "  2.3099653720855713,\n",
       "  3.252368688583374,\n",
       "  3.1319401264190674,\n",
       "  3.0337045192718506],\n",
       " 'cpu time': [16.185962438583374,\n",
       "  17.00124979019165,\n",
       "  17.301599502563477,\n",
       "  16.694705486297607,\n",
       "  16.470436811447144,\n",
       "  15.371254205703735,\n",
       "  16.349833726882935,\n",
       "  15.964494705200195,\n",
       "  16.079296588897705],\n",
       " 'accuracy': [0.7727708533077661,\n",
       "  0.7737296260786194,\n",
       "  0.716203259827421,\n",
       "  0.34132310642377756,\n",
       "  0.3087248322147651,\n",
       "  0.3288590604026846,\n",
       "  0.42665388302972196,\n",
       "  0.311601150527325,\n",
       "  0.3087248322147651],\n",
       " 'type': ['unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "prun_data = {\"percent\":[],\"f1\":[],\"cuda time\":[],\"cpu time\":[],\"accuracy\":[],\"type\":[],}\n",
    "for i in range(1,10):\n",
    "    print(f\"prun percent {i*10}%\")\n",
    "    metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "    model_prun_unstructure = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-cola', num_labels=2) \n",
    "    amt = i/10\n",
    "    prune.l1_unstructured(model_prun_unstructure.transformer.wte, name=\"weight\", amount=amt)\n",
    "    prune.l1_unstructured(model_prun_unstructure.transformer.wpe, name=\"weight\", amount=amt)\n",
    "\n",
    "    for layer_idx in range(12):\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].attn.c_attn, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].attn.c_proj, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].mlp.c_fc, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].mlp.c_proj, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].attn.c_attn, name=\"bias\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].attn.c_proj, name=\"bias\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].mlp.c_fc, name=\"bias\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.transformer.h[layer_idx].mlp.c_proj, name=\"bias\", amount=amt)\n",
    "\n",
    "        prune.remove(model_prun_unstructure.transformer.h[layer_idx].attn.c_attn, name=\"weight\")\n",
    "        prune.remove(model_prun_unstructure.transformer.h[layer_idx].attn.c_proj, name=\"weight\")\n",
    "        prune.remove(model_prun_unstructure.transformer.h[layer_idx].mlp.c_fc, name=\"weight\")\n",
    "        prune.remove(model_prun_unstructure.transformer.h[layer_idx].mlp.c_proj, name=\"weight\")\n",
    "\n",
    "    prune.l1_unstructured(model_prun_unstructure.transformer.ln_f, name=\"weight\", amount=amt)\n",
    "\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    metric_res = metric.compute()\n",
    "    print(metric_res)\n",
    "    prun_data[\"cuda time\"].append(end - start)\n",
    "    prun_data[\"f1\"].append(metric_res[\"f1\"])\n",
    "    prun_data[\"accuracy\"].append(metric_res[\"accuracy\"])\n",
    "    prun_data[\"type\"].append(\"unstructure\")\n",
    "    prun_data[\"percent\"].append(i*10)\n",
    "\n",
    "\n",
    "    device = \"cpu\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    prun_data[\"cpu time\"].append(end - start)\n",
    "\n",
    "prun_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bb311b6-2ce2-45f2-ac3c-6262afa5de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_cola_prun_unstructure.json\", \"w\") as json_file:\n",
    "    json.dump(prun_data, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_cola_dynamic_qint8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342b9d1-8391-4f06-b1f6-d5db112e8e75",
   "metadata": {},
   "source": [
    "##### Prun structure \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a400836-1f05-4478-b67b-2d990dde78dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prun percent 10%\n",
      "{'accuracy': 0.3096836049856184, 'f1': 0.002770083102493075}\n",
      "prun percent 20%\n",
      "{'accuracy': 0.3087248322147651, 'f1': 0.0}\n",
      "prun percent 30%\n",
      "{'accuracy': 0.3087248322147651, 'f1': 0.0}\n",
      "prun percent 40%\n",
      "{'accuracy': 0.3087248322147651, 'f1': 0.0}\n",
      "prun percent 50%\n",
      "{'accuracy': 0.3087248322147651, 'f1': 0.0}\n",
      "prun percent 60%\n",
      "{'accuracy': 0.3087248322147651, 'f1': 0.0}\n",
      "prun percent 70%\n",
      "{'accuracy': 0.41227229146692235, 'f1': 0.3789260385005066}\n",
      "prun percent 80%\n",
      "{'accuracy': 0.3087248322147651, 'f1': 0.0}\n",
      "prun percent 90%\n",
      "{'accuracy': 0.3087248322147651, 'f1': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'percent': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
       " 'f1': [0.002770083102493075,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.3789260385005066,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'cuda time': [2.8374617099761963,\n",
       "  2.929257869720459,\n",
       "  2.9206697940826416,\n",
       "  2.7994093894958496,\n",
       "  3.038015365600586,\n",
       "  3.015244960784912,\n",
       "  2.8812475204467773,\n",
       "  2.7551920413970947,\n",
       "  3.0102686882019043],\n",
       " 'cpu time': [16.874932289123535,\n",
       "  17.035096168518066,\n",
       "  16.986795663833618,\n",
       "  16.74958300590515,\n",
       "  17.316445112228394,\n",
       "  16.89370632171631,\n",
       "  17.10787296295166,\n",
       "  17.24095106124878,\n",
       "  16.980558395385742],\n",
       " 'accuracy': [0.3096836049856184,\n",
       "  0.3087248322147651,\n",
       "  0.3087248322147651,\n",
       "  0.3087248322147651,\n",
       "  0.3087248322147651,\n",
       "  0.3087248322147651,\n",
       "  0.41227229146692235,\n",
       "  0.3087248322147651,\n",
       "  0.3087248322147651],\n",
       " 'type': ['ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prun_data = {\"percent\":[],\"f1\":[],\"cuda time\":[],\"cpu time\":[],\"accuracy\":[],\"type\":[],}\n",
    "for i in range(1,10):\n",
    "    print(f\"prun percent {i*10}%\")\n",
    "    metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "    model_prun_structure = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-cola', num_labels=2) \n",
    "    amt = i/10\n",
    "\n",
    "    # print(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.query.weight)\n",
    "    \n",
    "    prune.ln_structured(model_prun_structure.transformer.wte, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "    prune.ln_structured(model_prun_structure.transformer.wpe, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "\n",
    "    for layer_idx in range(12):\n",
    "        # Access attention layers (query, key, value)\n",
    "        prune.ln_structured(model_prun_structure.transformer.h[layer_idx].attn.c_attn, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "        prune.ln_structured(model_prun_structure.transformer.h[layer_idx].attn.c_proj, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "        prune.ln_structured(model_prun_structure.transformer.h[layer_idx].mlp.c_fc, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "        prune.ln_structured(model_prun_structure.transformer.h[layer_idx].mlp.c_proj, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "        # prune.ln_structured(model_prun_unstructure.transformer.h[layer_idx].attn.c_attn, name=\"bias\", amount=amt,n=1,dim=0)\n",
    "        # prune.ln_structured(model_prun_unstructure.transformer.h[layer_idx].attn.c_proj, name=\"bias\", amount=amt,n=1,dim=0)\n",
    "        # prune.ln_structured(model_prun_unstructure.transformer.h[layer_idx].mlp.c_fc, name=\"bias\", amount=amt,n=1,dim=0)\n",
    "        # prune.ln_structured(model_prun_unstructure.transformer.h[layer_idx].mlp.c_proj, name=\"bias\", amount=amt,n=1,dim=0)\n",
    "\n",
    "        prune.remove(model_prun_structure.transformer.h[layer_idx].attn.c_attn, name=\"weight\")\n",
    "        prune.remove(model_prun_structure.transformer.h[layer_idx].attn.c_proj, name=\"weight\")\n",
    "        prune.remove(model_prun_structure.transformer.h[layer_idx].mlp.c_fc, name=\"weight\")\n",
    "        prune.remove(model_prun_structure.transformer.h[layer_idx].mlp.c_proj, name=\"weight\")\n",
    "\n",
    "    # prune.ln_structured(model_prun_unstructure.transformer.ln_f, name=\"weight\", amount=amt,n=1,dim=0)\n",
    "\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    model_prun_structure.to(device)\n",
    "    \n",
    "    model_prun_structure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_structure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    metric_res = metric.compute()\n",
    "    print(metric_res)\n",
    "    prun_data[\"cuda time\"].append(end - start)\n",
    "    prun_data[\"f1\"].append(metric_res[\"f1\"])\n",
    "    prun_data[\"accuracy\"].append(metric_res[\"accuracy\"])\n",
    "    prun_data[\"type\"].append(\"ln_structure\")\n",
    "    prun_data[\"percent\"].append(i*10)\n",
    "\n",
    "\n",
    "    device = \"cpu\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    prun_data[\"cpu time\"].append(end - start)\n",
    "\n",
    "prun_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9fbfb-f8b5-4e3f-b7c4-022a5f23b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "723e959f-d00d-4adf-a613-0c0e0aa2c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_cola_prun_structure.json\", \"w\") as json_file:\n",
    "    json.dump(prun_data, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_cola_dynamic_qint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7886e3f-7b75-40ea-9331-f2972fb8e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prun_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19487b-6e2f-41cd-aa86-57b748fd4f2d",
   "metadata": {},
   "source": [
    "### Flash Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ecc2f97-32b3-48d1-9716-04a04693326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5431447746883988,\n",
       " 'f1': 0.5904598195100988,\n",
       " 'cpu time': 13.992931842803955,\n",
       " 'cuda time': 2.488251209259033}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sdpa = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-cola', num_labels=2,attn_implementation=\"sdpa\") \n",
    "\n",
    "device = \"cpu\"\n",
    "model_sdpa.to(device)\n",
    "\n",
    "model_sdpa.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_sdpa(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "model_sdpa.to(device)\n",
    "\n",
    "model_sdpa.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_sdpa(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "\n",
    "res2[\"cuda time\"] = end - start\n",
    "\n",
    "res2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5bf8f24-1eb8-4f89-9904-a696fdd35d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5431447746883988,\n",
       " 'f1': 0.5904598195100988,\n",
       " 'cpu time': 13.992931842803955,\n",
       " 'cuda time': 2.488251209259033,\n",
       " 'size': 486.7061004638672}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_sdpa)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2\n",
    "\n",
    "# device = \"cuda\"\n",
    "# model_flash_attention.to(device)\n",
    "\n",
    "# model_flash_attention.eval()\n",
    "# start = time.time()\n",
    "# for batch in eval_dataloader:\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     # with torch.no_grad():\n",
    "#     with torch.inference_mode():\n",
    "#         # raise error if no optimized kernel is available\n",
    "#         with torch.backends.cuda.sdp_kernel(\n",
    "#             enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "#         ):\n",
    "#             outputs = model_flash_attention(**batch)\n",
    "#         # print(outputs)\n",
    "#         # break\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "# end = time.time()\n",
    "# # res2 = metric.compute()\n",
    "# res2[\"cuda time\"] = end - start\n",
    "# res2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05afd9e7-720d-49cb-920c-9de7f837ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_cola_sdpa.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_cola_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_sdpa.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_sdpa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e5904-1198-43b2-bdd8-f10b56bd9a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f371559-c376-465b-b689-afcdedf6ba56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ee36d-4d84-4f2c-b06e-b85e0d2c41c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d230898b-ddc9-4a67-aeee-badee2c63bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7775647171620326,\n",
       " 'f1': 0.8555417185554172,\n",
       " 'cpu time': 13.519189357757568,\n",
       " 'cuda time': 2.547826051712036}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eager = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-cola', num_labels=2,attn_implementation=\"sdpa\") \n",
    "\n",
    "device = \"cpu\"\n",
    "model_eager.to(device)\n",
    "\n",
    "model_eager.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_eager(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "model_eager.to(device)\n",
    "\n",
    "model_eager.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_eager(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2[\"cuda time\"] = end - start\n",
    "\n",
    "\n",
    "res2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce9d13e0-5ce4-4290-93d2-9d490a3cb63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7775647171620326,\n",
       " 'f1': 0.8555417185554172,\n",
       " 'cpu time': 13.519189357757568,\n",
       " 'cuda time': 2.547826051712036,\n",
       " 'size': 486.7061004638672}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_eager)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2\n",
    "\n",
    "# device = \"cuda\"\n",
    "# model_eager.to(device)\n",
    "\n",
    "# model_eager.eval()\n",
    "# start = time.time()\n",
    "# for batch in eval_dataloader:\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     # with torch.no_grad():\n",
    "#     with torch.inference_mode():\n",
    "#         # raise error if no optimized kernel is available\n",
    "#         with torch.backends.cuda.sdp_kernel(\n",
    "#             enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "#         ):\n",
    "#             outputs = model_eager(**batch)\n",
    "#         # print(outputs)\n",
    "#         # break\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "# end = time.time()\n",
    "# # res2 = metric.compute()\n",
    "# res2[\"cuda time\"] = end - start\n",
    "# res2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2da6cebc-1c28-4d62-9a5c-d0643fe0af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_cola_eager.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_cola_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_eager.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_eager, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ce3a6-a261-4b81-956d-e228cc57be69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "442d3b8f-4054-4562-93fb-aff9a513ccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7775647171620326,\n",
       " 'f1': 0.8555417185554172,\n",
       " 'cpu time': 13.519189357757568,\n",
       " 'cuda time': 3.119647264480591,\n",
       " 'size': 486.7061004638672}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_flash = GPT2ForSequenceClassification.from_pretrained('PavanNeerudu/gpt2-finetuned-cola', num_labels=2,attn_implementation=\"flash_attention_2\") \n",
    "model_flash.half()\n",
    "# device = \"cpu\"\n",
    "# model_flash.to(device)\n",
    "\n",
    "# model_flash.eval()\n",
    "# start = time.time()\n",
    "# for batch in eval_dataloader:\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     # with torch.no_grad():\n",
    "#     with torch.inference_mode():\n",
    "#         # raise error if no optimized kernel is available\n",
    "#         with torch.backends.cuda.sdp_kernel(\n",
    "#             enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "#         ):\n",
    "#             outputs = model_flash(**batch)\n",
    "#         # print(outputs)\n",
    "#         # break\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "# end = time.time()\n",
    "# res2 = metric.compute()\n",
    "# res2[\"cpu time\"] = end - start\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "model_flash.to(device)\n",
    "\n",
    "model_flash.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_flash(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2[\"cuda time\"] = end - start\n",
    "\n",
    "\n",
    "res2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a53c82d7-b414-4fe4-bd0b-4dd57713485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/gpt2_cola_flash.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204fbb3c-f6ff-4768-94f0-32164f644d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5c739-8cb8-46c7-b085-5e39ee4d20e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
