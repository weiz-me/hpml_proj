{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9742b9dc-1272-462e-b345-a1465ec8c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yes y |pip uninstall torch torchvision\n",
    "# !yes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe97718-7375-4469-96bb-1288cfb62938",
   "metadata": {},
   "source": [
    "# Fine-tune Bert MRPC\n",
    "\n",
    "Tutorial : https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217206aa-ac08-40e2-ab63-1544106dbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel,BertForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained('Intel/bert-base-uncased-mrpc')\n",
    "model = BertForSequenceClassification.from_pretrained(\"Intel/bert-base-uncased-mrpc\")\n",
    "text = \"The inspector analyzed the soundness in the building.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb0b44b-b076-4e00-8c99-c1baae733d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\",\"mrpc\")\n",
    "tokenizer = BertTokenizer.from_pretrained('Intel/bert-base-uncased-mrpc')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\",\"sentence2\",\"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\",\"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e06d589-ead3-4985-ae8d-dcfa8e8441e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle = True, batch_size=8, collate_fn = data_collator\n",
    ")\n",
    "train2_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn = data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], batch_size=8, collate_fn = data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e166eb-4637-42ec-89c5-62b45c769f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(),lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32991ee2-2b61-4347-ae95-128d2e103672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd19b2d-fcba-429c-a5c0-1a2c901481f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd14c7a8-0b82-4a66-912c-3db7dd1213df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8307246376811595,\n",
       " 'f1': 0.87943848059455,\n",
       " 'cuda time': 5.3341896533966064}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # encode_input = {}\n",
    "\n",
    "    # for i in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "    #     encode_input[i] = batch[i].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res = metric.compute()\n",
    "res[f\"{device} time\"] = end-start\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a5cbc0-adee-4c09-a158-141c10367810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8307246376811595,\n",
       " 'f1': 0.87943848059455,\n",
       " 'cuda time': 5.3341896533966064,\n",
       " 'cpu time': 64.74835252761841}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res[f\"cpu time\"] = end-start\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cb36d82-7a3b-4689-96e7-0bdb373d61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"./models/bert_mrpc.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ad108-ba53-4c00-9f65-65f5cdfe0bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca40889e-a67d-445c-84b0-3fd80d4ef375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8307246376811595,\n",
       " 'f1': 0.87943848059455,\n",
       " 'cuda time': 5.3341896533966064,\n",
       " 'cpu time': 64.74835252761841,\n",
       " 'size': 417.65528106689453}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "def get_model_size(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_size = param_size + buffer_size  # Total size in bytes\n",
    "    return total_size / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "size_in_mb = get_model_size(model)\n",
    "res[\"size\"] = size_in_mb\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc8de308-2c3d-4ed1-aab1-378f92018dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_mrpc.json\", \"w\") as json_file:\n",
    "    json.dump(res, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cc93d-ac0a-46ee-9b33-bca78e382e0f",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Tutorial: https://pytorch.org/tutorials/recipes/quantization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338a714-f281-4b81-827d-fa2da36fa2ef",
   "metadata": {},
   "source": [
    "#### dynamic quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9062752d-7031-41f3-bc3f-87c5b63cd30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"Intel/bert-base-uncased-mrpc\")\n",
    "\n",
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f5eb7a9-506d-4403-8179-acd558c959ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8234782608695652,\n",
       " 'f1': 0.874510611992582,\n",
       " 'cpu time': 41.94166564941406}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "model_dynamic_quantized_int8.eval()\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_dynamic_quantized_int8(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf606cba-3231-4323-9152-75fbe0fb5783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8234782608695652,\n",
       " 'f1': 0.874510611992582,\n",
       " 'cpu time': 41.94166564941406,\n",
       " 'cuda time': None,\n",
       " 'size': 91.080078125}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized_int8)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1ba54fe-980c-4246-83de-7262f14c8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_mrpc_dynamic_qint8.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_int8.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406450a-1a45-4395-b451-caadc2ca1df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f23ad728-8245-4956-92ac-54ee952a5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cpu\"\n",
    "model_dynamic_quantized_float16 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "424dbdf0-8513-46b8-aa7a-2ef785db2b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.831304347826087,\n",
       " 'f1': 0.8798017348203222,\n",
       " 'cpu time': 66.82392764091492}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_float16.to(device)\n",
    "\n",
    "model_dynamic_quantized_float16.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_dynamic_quantized_float16(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "195abb19-78f0-4a4d-b769-9e8695f8d16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.831304347826087,\n",
       " 'f1': 0.8798017348203222,\n",
       " 'cpu time': 66.82392764091492,\n",
       " 'cuda time': None,\n",
       " 'size': 91.080078125}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized_float16)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbbbcfb0-b174-4e41-a8d7-242b6f871885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_mrpc_dynamic_float16.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_float16.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bd5ec-d416-4e09-8bd0-33659c39b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f460d31-cf6c-4145-bfed-07d62df10178",
   "metadata": {},
   "source": [
    "### Model Prunning\n",
    "Tutorial: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c64094-4fae-43ff-bc12-fbb52b2a2dea",
   "metadata": {},
   "source": [
    "##### L1-Norm Unstructure Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad401f74-ee1a-42e0-8b34-d189bfafa37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "model_prun_unstructure = BertForSequenceClassification.from_pretrained(\"Intel/bert-base-uncased-mrpc\")\n",
    "# model.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6ee35c0-6660-4386-ad1c-fda10ce813c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prun percent 10%\n",
      "{'accuracy': 0.8284057971014492, 'f1': 0.8777869529314616}\n",
      "prun percent 20%\n",
      "{'accuracy': 0.8301449275362319, 'f1': 0.8797702092736972}\n",
      "prun percent 30%\n",
      "{'accuracy': 0.8330434782608696, 'f1': 0.8795986622073578}\n",
      "prun percent 40%\n",
      "{'accuracy': 0.8249275362318841, 'f1': 0.8743760399334443}\n",
      "prun percent 50%\n",
      "{'accuracy': 0.7582608695652174, 'f1': 0.8003829583532791}\n",
      "prun percent 60%\n",
      "{'accuracy': 0.3408695652173913, 'f1': 0.01728608470181504}\n",
      "prun percent 70%\n",
      "{'accuracy': 0.33507246376811595, 'f1': 0.0}\n",
      "prun percent 80%\n",
      "{'accuracy': 0.3356521739130435, 'f1': 0.0017421602787456446}\n",
      "prun percent 90%\n",
      "{'accuracy': 0.6608695652173913, 'f1': 0.7953830010493179}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'percent': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
       " 'f1': [0.8777869529314616,\n",
       "  0.8797702092736972,\n",
       "  0.8795986622073578,\n",
       "  0.8743760399334443,\n",
       "  0.8003829583532791,\n",
       "  0.01728608470181504,\n",
       "  0.0,\n",
       "  0.0017421602787456446,\n",
       "  0.7953830010493179],\n",
       " 'cuda time': [4.745062589645386,\n",
       "  5.883496284484863,\n",
       "  5.7819695472717285,\n",
       "  5.738783836364746,\n",
       "  5.856553077697754,\n",
       "  5.715821743011475,\n",
       "  5.7710862159729,\n",
       "  5.618096590042114,\n",
       "  5.6026082038879395],\n",
       " 'cpu time': [69.57550406455994,\n",
       "  65.29263353347778,\n",
       "  66.0870943069458,\n",
       "  65.65327739715576,\n",
       "  66.06130146980286,\n",
       "  65.49623966217041,\n",
       "  66.73524117469788,\n",
       "  65.88913536071777,\n",
       "  65.4354636669159],\n",
       " 'accuracy': [0.8284057971014492,\n",
       "  0.8301449275362319,\n",
       "  0.8330434782608696,\n",
       "  0.8249275362318841,\n",
       "  0.7582608695652174,\n",
       "  0.3408695652173913,\n",
       "  0.33507246376811595,\n",
       "  0.3356521739130435,\n",
       "  0.6608695652173913],\n",
       " 'type': ['unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure',\n",
       "  'unstructure']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "\n",
    "prun_data = {\"percent\":[],\"f1\":[],\"cuda time\":[],\"cpu time\":[],\"accuracy\":[],\"type\":[],}\n",
    "for i in range(1,10):\n",
    "    print(f\"prun percent {i*10}%\")\n",
    "    metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "    model_prun_unstructure = BertForSequenceClassification.from_pretrained(\"Intel/bert-base-uncased-mrpc\")\n",
    "\n",
    "    for layer_idx in range(12):\n",
    "        # Access attention layers (query, key, value)\n",
    "        amt = i/10\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.query, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.key, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.value, name=\"weight\", amount=amt)\n",
    "        \n",
    "        # Access feed-forward layers (intermediate dense layer)\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].intermediate.dense, name=\"weight\", amount=amt)\n",
    "        \n",
    "        # Optionally, prune the output dense layer (if desired)\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].output.dense, name=\"weight\", amount=amt)\n",
    "    \n",
    "    \n",
    "        # Access attention layers (query, key, value)\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.query, name=\"weight\")\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.key, name=\"weight\")\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.value, name=\"weight\")\n",
    "        \n",
    "        # Access feed-forward layers (intermediate dense layer)\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].intermediate.dense, name=\"weight\")\n",
    "        \n",
    "        # Optionally, prune the output dense layer (if desired)\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].output.dense, name=\"weight\")\n",
    "\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    metric_res = metric.compute()\n",
    "    print(metric_res)\n",
    "    prun_data[\"cuda time\"].append(end - start)\n",
    "    prun_data[\"f1\"].append(metric_res[\"f1\"])\n",
    "    prun_data[\"accuracy\"].append(metric_res[\"accuracy\"])\n",
    "    prun_data[\"type\"].append(\"unstructure\")\n",
    "    prun_data[\"percent\"].append(i*10)\n",
    "\n",
    "\n",
    "    device = \"cpu\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    prun_data[\"cpu time\"].append(end - start)\n",
    "\n",
    "prun_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bb311b6-2ce2-45f2-ac3c-6262afa5de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_mrpc_prun_unstructure.json\", \"w\") as json_file:\n",
    "    json.dump(prun_data, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342b9d1-8391-4f06-b1f6-d5db112e8e75",
   "metadata": {},
   "source": [
    "##### Prun structure \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a400836-1f05-4478-b67b-2d990dde78dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prun percent 10%\n",
      "{'accuracy': 0.6707246376811594, 'f1': 0.7916360968451944}\n",
      "prun percent 20%\n",
      "{'accuracy': 0.5907246376811595, 'f1': 0.7080231596360629}\n",
      "prun percent 30%\n",
      "{'accuracy': 0.5124637681159421, 'f1': 0.5552617662612375}\n",
      "prun percent 40%\n",
      "{'accuracy': 0.5669565217391305, 'f1': 0.6633618747183416}\n",
      "prun percent 50%\n",
      "{'accuracy': 0.5837681159420289, 'f1': 0.6973018549747049}\n",
      "prun percent 60%\n",
      "{'accuracy': 0.6202898550724638, 'f1': 0.7265135699373695}\n",
      "prun percent 70%\n",
      "{'accuracy': 0.6655072463768116, 'f1': 0.7990247300592128}\n",
      "prun percent 80%\n",
      "{'accuracy': 0.664927536231884, 'f1': 0.7987465181058496}\n",
      "prun percent 90%\n",
      "{'accuracy': 0.664927536231884, 'f1': 0.7987465181058496}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'percent': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
       " 'f1': [0.7916360968451944,\n",
       "  0.7080231596360629,\n",
       "  0.5552617662612375,\n",
       "  0.6633618747183416,\n",
       "  0.6973018549747049,\n",
       "  0.7265135699373695,\n",
       "  0.7990247300592128,\n",
       "  0.7987465181058496,\n",
       "  0.7987465181058496],\n",
       " 'cuda time': [5.699037790298462,\n",
       "  5.705916404724121,\n",
       "  5.637936353683472,\n",
       "  5.716999292373657,\n",
       "  6.346923828125,\n",
       "  5.590200901031494,\n",
       "  5.57944655418396,\n",
       "  5.609919786453247,\n",
       "  5.4983556270599365],\n",
       " 'cpu time': [65.66443920135498,\n",
       "  65.95729923248291,\n",
       "  68.50572896003723,\n",
       "  69.47684454917908,\n",
       "  66.88780951499939,\n",
       "  68.19040107727051,\n",
       "  66.8096559047699,\n",
       "  66.91059947013855,\n",
       "  66.54937934875488],\n",
       " 'accuracy': [0.6707246376811594,\n",
       "  0.5907246376811595,\n",
       "  0.5124637681159421,\n",
       "  0.5669565217391305,\n",
       "  0.5837681159420289,\n",
       "  0.6202898550724638,\n",
       "  0.6655072463768116,\n",
       "  0.664927536231884,\n",
       "  0.664927536231884],\n",
       " 'type': ['ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure',\n",
       "  'ln_structure']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prun_data = {\"percent\":[],\"f1\":[],\"cuda time\":[],\"cpu time\":[],\"accuracy\":[],\"type\":[],}\n",
    "for i in range(1,10):\n",
    "    print(f\"prun percent {i*10}%\")\n",
    "    metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "    model_prun_structure = BertForSequenceClassification.from_pretrained(\"Intel/bert-base-uncased-mrpc\")\n",
    "    amt = i/10\n",
    "\n",
    "    for layer_idx in range(12):\n",
    "        # Access attention layers (query, key, value)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.query, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.key, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.value, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "        \n",
    "        # Access feed-forward layers (intermediate dense layer)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].intermediate.dense, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "        \n",
    "        # Optionally, prune the output dense layer (if desired)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].output.dense, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "    \n",
    "    \n",
    "        # Access attention layers (query, key, value)\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.query, name=\"weight\")\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.key, name=\"weight\")\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.value, name=\"weight\")\n",
    "        \n",
    "        # Access feed-forward layers (intermediate dense layer)\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].intermediate.dense, name=\"weight\")\n",
    "        \n",
    "        # Optionally, prune the output dense layer (if desired)\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].output.dense, name=\"weight\")\n",
    "\n",
    "    # print(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.query.weight)\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    model_prun_structure.to(device)\n",
    "    \n",
    "    model_prun_structure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_structure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    metric_res = metric.compute()\n",
    "    print(metric_res)\n",
    "    prun_data[\"cuda time\"].append(end - start)\n",
    "    prun_data[\"f1\"].append(metric_res[\"f1\"])\n",
    "    prun_data[\"accuracy\"].append(metric_res[\"accuracy\"])\n",
    "    prun_data[\"type\"].append(\"ln_structure\")\n",
    "    prun_data[\"percent\"].append(i*10)\n",
    "\n",
    "\n",
    "    device = \"cpu\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    prun_data[\"cpu time\"].append(end - start)\n",
    "\n",
    "prun_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9fbfb-f8b5-4e3f-b7c4-022a5f23b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "723e959f-d00d-4adf-a613-0c0e0aa2c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_mprc_prun_structure.json\", \"w\") as json_file:\n",
    "    json.dump(prun_data, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7886e3f-7b75-40ea-9331-f2972fb8e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prun_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19487b-6e2f-41cd-aa86-57b748fd4f2d",
   "metadata": {},
   "source": [
    "### Flash Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc2f97-32b3-48d1-9716-04a04693326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from evaluate import load\n",
    "\n",
    "model_sdpa = BertForSequenceClassification.from_pretrained(\"Intel/bert-base-uncased-mrpc\" ,attn_implementation=\"sdpa\")\n",
    "metric = load(\"glue\",\"mrpc\")\n",
    "\n",
    "device = \"cpu\"\n",
    "model_sdpa.to(device)\n",
    "\n",
    "model_sdpa.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_sdpa(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf8f24-1eb8-4f89-9904-a696fdd35d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\"\n",
    "metric = load(\"glue\",\"mrpc\")\n",
    "\n",
    "model_sdpa.to(device)\n",
    "\n",
    "model_sdpa.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_sdpa(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "# res2 = metric.compute()\n",
    "res2[\"cuda time\"] = end - start\n",
    "res2\n",
    "size_in_mb = get_model_size(model_qat)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afd9e7-720d-49cb-920c-9de7f837ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "with open(\"results/bert_mrpc_sdpa.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_sdpa.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_sdpa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e5904-1198-43b2-bdd8-f10b56bd9a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f371559-c376-465b-b689-afcdedf6ba56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ee36d-4d84-4f2c-b06e-b85e0d2c41c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d230898b-ddc9-4a67-aeee-badee2c63bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8307246376811595,\n",
       " 'f1': 0.87943848059455,\n",
       " 'cpu time': 67.77916312217712}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eager = BertForSequenceClassification.from_pretrained(\"Intel/bert-base-uncased-mrpc\" ,attn_implementation=\"eager\")\n",
    "\n",
    "device = \"cpu\"\n",
    "model_eager.to(device)\n",
    "metric = load(\"glue\",\"mrpc\")\n",
    "\n",
    "model_eager.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_eager(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce9d13e0-5ce4-4290-93d2-9d490a3cb63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8307246376811595,\n",
       " 'f1': 0.87943848059455,\n",
       " 'cpu time': 67.77916312217712,\n",
       " 'cuda time': 6.1416239738464355}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\"\n",
    "model_eager.to(device)\n",
    "metric = load(\"glue\",\"mrpc\")\n",
    "\n",
    "model_eager.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_eager(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "# res2 = metric.compute()\n",
    "res2[\"cuda time\"] = end - start\n",
    "res2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2da6cebc-1c28-4d62-9a5c-d0643fe0af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_mrpc_eager.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_eager.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_eager, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ce3a6-a261-4b81-956d-e228cc57be69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
