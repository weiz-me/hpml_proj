{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9742b9dc-1272-462e-b345-a1465ec8c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yes y |pip uninstall torch torchvision\n",
    "# !yes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe97718-7375-4469-96bb-1288cfb62938",
   "metadata": {},
   "source": [
    "# Fine-tune T5 cola\n",
    "\n",
    "Tutorial : https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217206aa-ac08-40e2-ab63-1544106dbde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('PavanNeerudu/t5-base-finetuned-cola')\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"PavanNeerudu/t5-base-finetuned-cola\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af336fa6-1348-4fe8-bf63-6b96226bd58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0, 9961,    1])\n"
     ]
    }
   ],
   "source": [
    "text = \"cola sentence: \" + \"The book on the table is mine.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "generated_ids = model.generate(**encoded_input)\n",
    "output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_ids[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ddb3e13-978f-44d7-9b2a-fb0122c6d016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'acceptable'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb0b44b-b076-4e00-8c99-c1baae733d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62124c87c9db4fbfba34374476960c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  33%|###2      | 294M/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 8551\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1043\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1063\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\",\"cola\")\n",
    "tokenizer = T5Tokenizer.from_pretrained('PavanNeerudu/t5-base-finetuned-cola')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    text_list = [\"cola sentence1: \" + examples[\"sentence\"][i] for i in range(len(examples[\"sentence\"]))]\n",
    "    return tokenizer(text_list, truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\",\"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\",\"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e06d589-ead3-4985-ae8d-dcfa8e8441e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle = True, batch_size=8, collate_fn = data_collator\n",
    ")\n",
    "train2_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn = data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn = data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e166eb-4637-42ec-89c5-62b45c769f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AdamW\n",
    "# optimizer = AdamW(model.parameters(),lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32991ee2-2b61-4347-ae95-128d2e103672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd19b2d-fcba-429c-a5c0-1a2c901481f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd14c7a8-0b82-4a66-912c-3db7dd1213df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8312559923298178,\n",
       " 'f1': 0.8828229027962716,\n",
       " 'cuda time': 11.242987871170044}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # encode_input = {}\n",
    "\n",
    "    # for i in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "    #     encode_input[i] = batch[i].to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**batch)\n",
    "        output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # print(output_text)\n",
    "    \n",
    "    # logits = outputs.logits\n",
    "    # predictions = torch.argmax(logits, dim=-1)\n",
    "    predictions = [1 if output_text[i] == \"acceptable\" else 0 for i in range(len(output_text))]\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res = metric.compute()\n",
    "res[f\"{device} time\"] = end-start\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a5cbc0-adee-4c09-a158-141c10367810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8312559923298178,\n",
       " 'f1': 0.8828229027962716,\n",
       " 'cuda time': 11.242987871170044,\n",
       " 'cpu time': 38.5801739692688}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**batch)\n",
    "        output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        # output_text = [tokenizer.decode(generated_ids[i], skip_special_tokens=True) for i in range(len(generated_ids))]\n",
    "        \n",
    "        # print(output_text)\n",
    "    \n",
    "    # logits = outputs.logits\n",
    "    # predictions = torch.argmax(logits, dim=-1)\n",
    "    predictions = [1 if output_text[i] == \"acceptable\" else 0 for i in range(len(output_text))]\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res[f\"cpu time\"] = end-start\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cb36d82-7a3b-4689-96e7-0bdb373d61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"./models/t5_cola.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a9ad108-ba53-4c00-9f65-65f5cdfe0bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_size(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_size = param_size + buffer_size  # Total size in bytes\n",
    "    return total_size / (1024 ** 2)  # Convert to MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca40889e-a67d-445c-84b0-3fd80d4ef375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8312559923298178,\n",
       " 'f1': 0.8828229027962716,\n",
       " 'cuda time': 11.242987871170044,\n",
       " 'cpu time': 38.5801739692688,\n",
       " 'size': 850.3095703125}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "def get_model_size(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_size = param_size + buffer_size  # Total size in bytes\n",
    "    return total_size / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "size_in_mb = get_model_size(model)\n",
    "res[\"size\"] = size_in_mb\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc8de308-2c3d-4ed1-aab1-378f92018dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/t5_cola.json\", \"w\") as json_file:\n",
    "    json.dump(res, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cc93d-ac0a-46ee-9b33-bca78e382e0f",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Tutorial: https://pytorch.org/tutorials/recipes/quantization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338a714-f281-4b81-827d-fa2da36fa2ef",
   "metadata": {},
   "source": [
    "#### dynamic quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9062752d-7031-41f3-bc3f-87c5b63cd30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"PavanNeerudu/t5-base-finetuned-cola\")\n",
    "\n",
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f5eb7a9-506d-4403-8179-acd558c959ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8125599232981783,\n",
       " 'f1': 0.8753586228881096,\n",
       " 'cpu time': 16.79375648498535}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "model_dynamic_quantized_int8.eval()\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model_dynamic_quantized_int8.generate(**batch)\n",
    "        output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        # output_text = [tokenizer.decode(generated_ids[i], skip_special_tokens=True) for i in range(len(generated_ids))]\n",
    "        \n",
    "        # print(output_text)\n",
    "    \n",
    "    # logits = outputs.logits\n",
    "    # predictions = torch.argmax(logits, dim=-1)\n",
    "    predictions = [1 if output_text[i] == \"acceptable\" else 0 for i in range(len(output_text))]\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf606cba-3231-4323-9152-75fbe0fb5783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8125599232981783,\n",
       " 'f1': 0.8753586228881096,\n",
       " 'cpu time': 16.79375648498535,\n",
       " 'cuda time': None,\n",
       " 'size': 94.3095703125}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized_int8)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1ba54fe-980c-4246-83de-7262f14c8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/t5_cola_dynamic_qint8.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/t5_cola_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_int8.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406450a-1a45-4395-b451-caadc2ca1df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f23ad728-8245-4956-92ac-54ee952a5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cpu\"\n",
    "model_dynamic_quantized_float16 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "424dbdf0-8513-46b8-aa7a-2ef785db2b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8312559923298178,\n",
       " 'f1': 0.8828229027962716,\n",
       " 'cpu time': 26.936930179595947}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_float16.to(device)\n",
    "\n",
    "model_dynamic_quantized_float16.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model_dynamic_quantized_float16.generate(**batch)\n",
    "        output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        # output_text = [tokenizer.decode(generated_ids[i], skip_special_tokens=True) for i in range(len(generated_ids))]\n",
    "        \n",
    "        # print(output_text)\n",
    "    \n",
    "    # logits = outputs.logits\n",
    "    # predictions = torch.argmax(logits, dim=-1)\n",
    "    predictions = [1 if output_text[i] == \"acceptable\" else 0 for i in range(len(output_text))] \n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "195abb19-78f0-4a4d-b769-9e8695f8d16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8312559923298178,\n",
       " 'f1': 0.8828229027962716,\n",
       " 'cpu time': 26.936930179595947,\n",
       " 'cuda time': None,\n",
       " 'size': 94.3095703125}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized_float16)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbbbcfb0-b174-4e41-a8d7-242b6f871885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/t5_cola_dynamic_float16.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/t5_cola_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_float16.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bd5ec-d416-4e09-8bd0-33659c39b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f460d31-cf6c-4145-bfed-07d62df10178",
   "metadata": {},
   "source": [
    "### Model Prunning\n",
    "Tutorial: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c64094-4fae-43ff-bc12-fbb52b2a2dea",
   "metadata": {},
   "source": [
    "##### L1-Norm Unstructure Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad401f74-ee1a-42e0-8b34-d189bfafa37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "model_prun_unstructure = T5ForConditionalGeneration.from_pretrained(\"PavanNeerudu/t5-base-finetuned-cola\")\n",
    "# model.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9181e2e4-d94f-4f72-8074-611450fdae43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_prun_unstructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ee35c0-6660-4386-ad1c-fda10ce813c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prun percent 10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6740172579098753, 'f1': 0.7680763983628922}\n",
      "prun percent 20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5407478427612655, 'f1': 0.6266562743569758}\n",
      "prun percent 30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.3796740172579099, 'f1': 0.30802139037433157}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'percent': [10, 20, 30],\n",
       " 'f1': [0.7680763983628922, 0.6266562743569758, 0.30802139037433157],\n",
       " 'cuda time': [20.210673093795776, 41.29215908050537, 53.83593821525574],\n",
       " 'cpu time': [],\n",
       " 'accuracy': [0.6740172579098753, 0.5407478427612655, 0.3796740172579099],\n",
       " 'type': ['unstructure', 'unstructure', 'unstructure']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "\n",
    "prun_data = {\"percent\":[],\"f1\":[],\"cuda time\":[],\"cpu time\":[],\"accuracy\":[],\"type\":[],}\n",
    "for i in range(1,4):\n",
    "    print(f\"prun percent {i*10}%\")\n",
    "    metric = load(\"glue\",config_name=\"mrpc\")\n",
    "    pruning_amount=i/10\n",
    "    model_prun_unstructure = T5ForConditionalGeneration.from_pretrained(\"PavanNeerudu/t5-base-finetuned-cola\")\n",
    "    \n",
    "    for name, module in model_prun_unstructure.named_modules():\n",
    "        if hasattr(module, \"weight\"):  # Check if the module has a weight parameter\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=pruning_amount)\n",
    "            prune.remove(module, name=\"weight\")\n",
    "\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            generated_ids = model_prun_unstructure.generate(**batch)\n",
    "            output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "            # output_text = [tokenizer.decode(generated_ids[i], skip_special_tokens=True) for i in range(len(generated_ids))]\n",
    "            \n",
    "            # print(output_text)\n",
    "        \n",
    "        # logits = outputs.logits\n",
    "        # predictions = torch.argmax(logits, dim=-1)\n",
    "        predictions = [1 if output_text[i] == \"acceptable\" else 0 for i in range(len(output_text))] \n",
    "        \n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    metric_res = metric.compute()\n",
    "    print(metric_res)\n",
    "    prun_data[\"cuda time\"].append(end - start)\n",
    "    prun_data[\"f1\"].append(metric_res[\"f1\"])\n",
    "    prun_data[\"accuracy\"].append(metric_res[\"accuracy\"])\n",
    "    prun_data[\"type\"].append(\"unstructure\")\n",
    "    prun_data[\"percent\"].append(i*10)\n",
    "\n",
    "\n",
    "    # device = \"cpu\"\n",
    "    # model_prun_unstructure.to(device)\n",
    "    \n",
    "    # model_prun_unstructure.eval()\n",
    "    # start = time.time()\n",
    "    # for batch in eval_dataloader:\n",
    "    #     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    #     with torch.no_grad():\n",
    "    \n",
    "    #         generated_ids = model_prun_unstructure.generate(**batch)\n",
    "    #         output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    #         # output_text = [tokenizer.decode(generated_ids[i], skip_special_tokens=True) for i in range(len(generated_ids))]\n",
    "            \n",
    "    #         # print(output_text)\n",
    "        \n",
    "    #     # logits = outputs.logits\n",
    "    #     # predictions = torch.argmax(logits, dim=-1)\n",
    "    #     predictions = [1 if output_text[i] == \"acceptable\" else 0 for i in range(len(output_text))] \n",
    "        \n",
    "    #     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    # end = time.time()\n",
    "    # prun_data[\"cpu time\"].append(end - start)\n",
    "\n",
    "prun_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a4163-2d01-4aa8-8732-483c879b96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prun_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bb311b6-2ce2-45f2-ac3c-6262afa5de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/t5_cola_prun_unstructure.json\", \"w\") as json_file:\n",
    "    json.dump(prun_data, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/t5_cola_dynamic_qint8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342b9d1-8391-4f06-b1f6-d5db112e8e75",
   "metadata": {},
   "source": [
    "##### Prun structure \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a400836-1f05-4478-b67b-2d990dde78dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prun percent 10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.3087248322147651, 'f1': 0.0}\n",
      "prun percent 20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.3087248322147651, 'f1': 0.0}\n",
      "prun percent 30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.3087248322147651, 'f1': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'percent': [10, 20, 30],\n",
       " 'f1': [0.0, 0.0, 0.0],\n",
       " 'cuda time': [6.5053088665008545, 14.838896989822388, 39.54865026473999],\n",
       " 'cpu time': [],\n",
       " 'accuracy': [0.3087248322147651, 0.3087248322147651, 0.3087248322147651],\n",
       " 'type': ['ln_structure', 'ln_structure', 'ln_structure']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prun_data = {\"percent\":[],\"f1\":[],\"cuda time\":[],\"cpu time\":[],\"accuracy\":[],\"type\":[],}\n",
    "for i in range(1,4):\n",
    "    print(f\"prun percent {i*10}%\")\n",
    "    metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "    model_prun_structure = T5ForConditionalGeneration.from_pretrained(\"PavanNeerudu/t5-base-finetuned-cola\")\n",
    "    amt = i/10\n",
    "\n",
    "    for name, module in model_prun_structure.named_modules():\n",
    "        if hasattr(module, \"weight\") and module.weight.ndim > 1:  # Check if the module has a weight parameter\n",
    "            prune.ln_structured(module, name=\"weight\", amount=amt, n=2,dim=0)\n",
    "            prune.remove(module, name=\"weight\")\n",
    "\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    model_prun_structure.to(device)\n",
    "    \n",
    "    model_prun_structure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            generated_ids = model_prun_structure.generate(**batch)\n",
    "            output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "            # output_text = [tokenizer.decode(generated_ids[i], skip_special_tokens=True) for i in range(len(generated_ids))]\n",
    "            \n",
    "            # print(output_text)\n",
    "        \n",
    "        # logits = outputs.logits\n",
    "        # predictions = torch.argmax(logits, dim=-1)\n",
    "        predictions = [1 if output_text[i] == \"acceptable\" else 0 for i in range(len(output_text))] \n",
    "        \n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    metric_res = metric.compute()\n",
    "    print(metric_res)\n",
    "    prun_data[\"cuda time\"].append(end - start)\n",
    "    prun_data[\"f1\"].append(metric_res[\"f1\"])\n",
    "    prun_data[\"accuracy\"].append(metric_res[\"accuracy\"])\n",
    "    prun_data[\"type\"].append(\"ln_structure\")\n",
    "    prun_data[\"percent\"].append(i*10)\n",
    "\n",
    "\n",
    "    \n",
    "    # device = \"cpu\"\n",
    "    # model_prun_structure.to(device)\n",
    "    \n",
    "    # model_prun_structure.eval()\n",
    "    # start = time.time()\n",
    "    # for batch in eval_dataloader:\n",
    "    #     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    #     with torch.no_grad():\n",
    "    \n",
    "    #         generated_ids = model_prun_structure.generate(**batch)\n",
    "    #         output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    #         # output_text = [tokenizer.decode(generated_ids[i], skip_special_tokens=True) for i in range(len(generated_ids))]\n",
    "            \n",
    "    #         # print(output_text)\n",
    "        \n",
    "    #     # logits = outputs.logits\n",
    "    #     # predictions = torch.argmax(logits, dim=-1)\n",
    "    #     predictions = [1 if output_text[i] == \"acceptable\" else 0 for i in range(len(output_text))] \n",
    "        \n",
    "    #     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    # end = time.time()\n",
    "    # prun_data[\"cpu time\"].append(end - start)\n",
    "\n",
    "prun_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9fbfb-f8b5-4e3f-b7c4-022a5f23b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "723e959f-d00d-4adf-a613-0c0e0aa2c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_mprc_prun_structure.json\", \"w\") as json_file:\n",
    "    json.dump(prun_data, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/t5_cola_dynamic_qint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7886e3f-7b75-40ea-9331-f2972fb8e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prun_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19487b-6e2f-41cd-aa86-57b748fd4f2d",
   "metadata": {},
   "source": [
    "### Flash Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ecc2f97-32b3-48d1-9716-04a04693326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from evaluate import load\n",
    "\n",
    "# model_sdpa = T5ForConditionalGeneration.from_pretrained(\"PavanNeerudu/t5-base-finetuned-cola\" ,attn_implementation=\"sdpa\")\n",
    "# metric = load(\"glue\",\"cola\")\n",
    "\n",
    "# device = \"cpu\"\n",
    "# model_sdpa.to(device)\n",
    "\n",
    "# model_sdpa.eval()\n",
    "# start = time.time()\n",
    "# for batch in eval_dataloader:\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     # with torch.no_grad():\n",
    "#     with torch.inference_mode():\n",
    "#         # raise error if no optimized kernel is available\n",
    "#         with torch.backends.cuda.sdp_kernel(\n",
    "#             enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "#         ):\n",
    "#             outputs = model_sdpa(**batch)\n",
    "#         # print(outputs)\n",
    "#         # break\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "# end = time.time()\n",
    "# res2 = metric.compute()\n",
    "# res2[\"cpu time\"] = end - start\n",
    "# res2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5bf8f24-1eb8-4f89-9904-a696fdd35d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_qat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m res2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     25\u001b[0m res2\n\u001b[0;32m---> 26\u001b[0m size_in_mb \u001b[38;5;241m=\u001b[39m get_model_size(\u001b[43mmodel_qat\u001b[49m)\n\u001b[1;32m     27\u001b[0m res2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m size_in_mb\n\u001b[1;32m     28\u001b[0m res2\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_qat' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# device = \"cuda\"\n",
    "# metric = load(\"glue\",\"cola\")\n",
    "\n",
    "# model_sdpa.to(device)\n",
    "\n",
    "# model_sdpa.eval()\n",
    "# start = time.time()\n",
    "# for batch in eval_dataloader:\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     # with torch.no_grad():\n",
    "#     with torch.inference_mode():\n",
    "#         # raise error if no optimized kernel is available\n",
    "#         with torch.backends.cuda.sdp_kernel(\n",
    "#             enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "#         ):\n",
    "#             outputs = model_sdpa(**batch)\n",
    "#         # print(outputs)\n",
    "#         # break\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "# end = time.time()\n",
    "# # res2 = metric.compute()\n",
    "# res2[\"cuda time\"] = end - start\n",
    "# res2\n",
    "# size_in_mb = get_model_size(model_qat)\n",
    "# res2[\"size\"] = size_in_mb\n",
    "# res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afd9e7-720d-49cb-920c-9de7f837ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pickle\n",
    "# with open(\"results/t5_cola_sdpa.json\", \"w\") as json_file:\n",
    "#     json.dump(res2, json_file, indent=4)\n",
    "# # torch.save(model_dynamic_quantized, \"./models/t5_cola_dynamic_qint8\")\n",
    "\n",
    "# # with open(\"./models/bert_sdpa.pkl\", \"wb\") as f:\n",
    "# #     pickle.dump(model_sdpa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e5904-1198-43b2-bdd8-f10b56bd9a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f371559-c376-465b-b689-afcdedf6ba56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ee36d-4d84-4f2c-b06e-b85e0d2c41c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d230898b-ddc9-4a67-aeee-badee2c63bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/home/wei/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'matthews_correlation': np.float64(0.5891424967516642),\n",
       " 'cpu time': 33.72944641113281}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eager = T5ForConditionalGeneration.from_pretrained(\"PavanNeerudu/t5-base-finetuned-cola\" ,attn_implementation=\"eager\")\n",
    "\n",
    "device = \"cpu\"\n",
    "model_eager.to(device)\n",
    "metric = load(\"glue\",\"cola\")\n",
    "\n",
    "model_eager.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            \n",
    "            generated_ids = model_eager.generate(**batch)\n",
    "            output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    predictions = [1 if output_text[i] == \"acceptable\" else 0 for i in range(len(output_text))] \n",
    "        \n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce9d13e0-5ce4-4290-93d2-9d490a3cb63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/home/wei/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'matthews_correlation': np.float64(0.5891424967516642),\n",
       " 'cpu time': 33.72944641113281,\n",
       " 'cuda time': 7.704435586929321}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\"\n",
    "model_eager.to(device)\n",
    "metric = load(\"glue\",\"cola\")\n",
    "\n",
    "model_eager.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    " \n",
    "            generated_ids = model_eager.generate(**batch)\n",
    "            output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    predictions = [1 if output_text[i] == \"acceptable\" else 0 for i in range(len(output_text))] \n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "# res2 = metric.compute()\n",
    "res2[\"cuda time\"] = end - start\n",
    "res2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2da6cebc-1c28-4d62-9a5c-d0643fe0af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/t5_cola_eager.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/t5_cola_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_eager.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_eager, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ce3a6-a261-4b81-956d-e228cc57be69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
