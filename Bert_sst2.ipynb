{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9742b9dc-1272-462e-b345-a1465ec8c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yes y |pip uninstall torch torchvision\n",
    "# !yes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe97718-7375-4469-96bb-1288cfb62938",
   "metadata": {},
   "source": [
    "# Fine-tune Bert sst2\n",
    "\n",
    "Tutorial : https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217206aa-ac08-40e2-ab63-1544106dbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel,BertForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained('pmthangk09/bert-base-uncased-glue-sst2')\n",
    "model = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-sst2\")\n",
    "text = \"The inspector analyzed the soundness in the building.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd6331-af91-4be5-902f-11f993bd26be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb0b44b-b076-4e00-8c99-c1baae733d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\",\"sst2\")\n",
    "tokenizer = BertTokenizer.from_pretrained('pmthangk09/bert-base-uncased-glue-sst2')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\",\"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\",\"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e06d589-ead3-4985-ae8d-dcfa8e8441e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle = True, batch_size=8, collate_fn = data_collator\n",
    ")\n",
    "train2_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn = data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], batch_size=4, collate_fn = data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cce253-c2fd-4c0e-9322-a252653de9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for b in train_dataloader:\n",
    "#     print(b[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e166eb-4637-42ec-89c5-62b45c769f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(),lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32991ee2-2b61-4347-ae95-128d2e103672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd19b2d-fcba-429c-a5c0-1a2c901481f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd14c7a8-0b82-4a66-912c-3db7dd1213df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m metric\u001b[38;5;241m.\u001b[39madd_batch(predictions\u001b[38;5;241m=\u001b[39mpredictions, references\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/usr/lib/python3.10/dataclasses.py:239\u001b[0m, in \u001b[0;36m_recursive_repr.<locals>.wrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m repr_running\u001b[38;5;241m.\u001b[39madd(key)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43muser_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     repr_running\u001b[38;5;241m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:523\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    520\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    521\u001b[0m     )\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor_str.py:708\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    707\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 708\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor_str.py:625\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    623\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    624\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 625\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    628\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor_str.py:357\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    355\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor_str.py:146\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(\n\u001b[0;32m--> 146\u001b[0m         tensor_view, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m tensor_view\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # encode_input = {}\n",
    "\n",
    "    # for i in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "    #     encode_input[i] = batch[i].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        print(\n",
    "    logits = outputs.logits\n",
    "    # print(outputs)\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res = metric.compute()\n",
    "res[f\"{device} time\"] = end-start\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a5cbc0-adee-4c09-a158-141c10367810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "metric = load(\"glue\",config_name=\"sst2\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res[f\"cpu time\"] = end-start\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb36d82-7a3b-4689-96e7-0bdb373d61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"./models/bert_sst2.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ad108-ba53-4c00-9f65-65f5cdfe0bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40889e-a67d-445c-84b0-3fd80d4ef375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_model_size(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_size = param_size + buffer_size  # Total size in bytes\n",
    "    return total_size / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "size_in_mb = get_model_size(model)\n",
    "res[\"size\"] = size_in_mb\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8de308-2c3d-4ed1-aab1-378f92018dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_sst2.json\", \"w\") as json_file:\n",
    "    json.dump(res, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cc93d-ac0a-46ee-9b33-bca78e382e0f",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Tutorial: https://pytorch.org/tutorials/recipes/quantization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338a714-f281-4b81-827d-fa2da36fa2ef",
   "metadata": {},
   "source": [
    "#### dynamic quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9062752d-7031-41f3-bc3f-87c5b63cd30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-sst2\")\n",
    "\n",
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5eb7a9-506d-4403-8179-acd558c959ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "model_dynamic_quantized_int8.eval()\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_dynamic_quantized_int8(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf606cba-3231-4323-9152-75fbe0fb5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized_int8)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba54fe-980c-4246-83de-7262f14c8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_sst2_dynamic_qint8.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_sst2_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_int8.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406450a-1a45-4395-b451-caadc2ca1df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ad728-8245-4956-92ac-54ee952a5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cpu\"\n",
    "model_dynamic_quantized_float16 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424dbdf0-8513-46b8-aa7a-2ef785db2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_float16.to(device)\n",
    "\n",
    "model_dynamic_quantized_float16.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_dynamic_quantized_float16(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195abb19-78f0-4a4d-b769-9e8695f8d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized_float16)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbbcfb0-b174-4e41-a8d7-242b6f871885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_sst2_dynamic_float16.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_sst2_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_float16.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bd5ec-d416-4e09-8bd0-33659c39b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f460d31-cf6c-4145-bfed-07d62df10178",
   "metadata": {},
   "source": [
    "### Model Prunning\n",
    "Tutorial: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c64094-4fae-43ff-bc12-fbb52b2a2dea",
   "metadata": {},
   "source": [
    "##### L1-Norm Unstructure Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad401f74-ee1a-42e0-8b34-d189bfafa37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "model_prun_unstructure = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-sst2\")\n",
    "# model.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee35c0-6660-4386-ad1c-fda10ce813c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "\n",
    "prun_data = {\"percent\":[],\"f1\":[],\"cuda time\":[],\"cpu time\":[],\"accuracy\":[],\"type\":[],}\n",
    "for i in range(1,10):\n",
    "    print(f\"prun percent {i*10}%\")\n",
    "    metric = load(\"glue\",config_name=\"sst2\")\n",
    "\n",
    "    model_prun_unstructure = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-sst2\")\n",
    "\n",
    "    for layer_idx in range(12):\n",
    "        # Access attention layers (query, key, value)\n",
    "        amt = i/10\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.query, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.key, name=\"weight\", amount=amt)\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.value, name=\"weight\", amount=amt)\n",
    "        \n",
    "        # Access feed-forward layers (intermediate dense layer)\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].intermediate.dense, name=\"weight\", amount=amt)\n",
    "        \n",
    "        # Optionally, prune the output dense layer (if desired)\n",
    "        prune.l1_unstructured(model_prun_unstructure.bert.encoder.layer[layer_idx].output.dense, name=\"weight\", amount=amt)\n",
    "    \n",
    "    \n",
    "        # Access attention layers (query, key, value)\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.query, name=\"weight\")\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.key, name=\"weight\")\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].attention.self.value, name=\"weight\")\n",
    "        \n",
    "        # Access feed-forward layers (intermediate dense layer)\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].intermediate.dense, name=\"weight\")\n",
    "        \n",
    "        # Optionally, prune the output dense layer (if desired)\n",
    "        prune.remove(model_prun_unstructure.bert.encoder.layer[layer_idx].output.dense, name=\"weight\")\n",
    "\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    metric_res = metric.compute()\n",
    "    print(metric_res)\n",
    "    prun_data[\"cuda time\"].append(end - start)\n",
    "    prun_data[\"f1\"].append(metric_res[\"f1\"])\n",
    "    prun_data[\"accuracy\"].append(metric_res[\"accuracy\"])\n",
    "    prun_data[\"type\"].append(\"unstructure\")\n",
    "    prun_data[\"percent\"].append(i*10)\n",
    "\n",
    "\n",
    "    device = \"cpu\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    prun_data[\"cpu time\"].append(end - start)\n",
    "\n",
    "prun_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb311b6-2ce2-45f2-ac3c-6262afa5de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_sst2_prun_unstructure.json\", \"w\") as json_file:\n",
    "    json.dump(prun_data, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_sst2_dynamic_qint8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342b9d1-8391-4f06-b1f6-d5db112e8e75",
   "metadata": {},
   "source": [
    "##### Prun structure \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a400836-1f05-4478-b67b-2d990dde78dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prun_data = {\"percent\":[],\"f1\":[],\"cuda time\":[],\"cpu time\":[],\"accuracy\":[],\"type\":[],}\n",
    "for i in range(1,10):\n",
    "    print(f\"prun percent {i*10}%\")\n",
    "    metric = load(\"glue\",config_name=\"sst2\")\n",
    "\n",
    "    model_prun_structure = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-sst2\")\n",
    "    amt = i/10\n",
    "\n",
    "    for layer_idx in range(12):\n",
    "        # Access attention layers (query, key, value)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.query, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.key, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.value, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "        \n",
    "        # Access feed-forward layers (intermediate dense layer)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].intermediate.dense, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "        \n",
    "        # Optionally, prune the output dense layer (if desired)\n",
    "        prune.ln_structured(model_prun_structure.bert.encoder.layer[layer_idx].output.dense, name=\"weight\", amount=amt,n=2,dim=0)\n",
    "    \n",
    "    \n",
    "        # Access attention layers (query, key, value)\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.query, name=\"weight\")\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.key, name=\"weight\")\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.value, name=\"weight\")\n",
    "        \n",
    "        # Access feed-forward layers (intermediate dense layer)\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].intermediate.dense, name=\"weight\")\n",
    "        \n",
    "        # Optionally, prune the output dense layer (if desired)\n",
    "        prune.remove(model_prun_structure.bert.encoder.layer[layer_idx].output.dense, name=\"weight\")\n",
    "\n",
    "    # print(model_prun_structure.bert.encoder.layer[layer_idx].attention.self.query.weight)\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    model_prun_structure.to(device)\n",
    "    \n",
    "    model_prun_structure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_structure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    metric_res = metric.compute()\n",
    "    print(metric_res)\n",
    "    prun_data[\"cuda time\"].append(end - start)\n",
    "    prun_data[\"f1\"].append(metric_res[\"f1\"])\n",
    "    prun_data[\"accuracy\"].append(metric_res[\"accuracy\"])\n",
    "    prun_data[\"type\"].append(\"ln_structure\")\n",
    "    prun_data[\"percent\"].append(i*10)\n",
    "\n",
    "\n",
    "    device = \"cpu\"\n",
    "    model_prun_unstructure.to(device)\n",
    "    \n",
    "    model_prun_unstructure.eval()\n",
    "    start = time.time()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            outputs = model_prun_unstructure(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    end = time.time()\n",
    "    prun_data[\"cpu time\"].append(end - start)\n",
    "\n",
    "prun_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9fbfb-f8b5-4e3f-b7c4-022a5f23b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723e959f-d00d-4adf-a613-0c0e0aa2c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_mprc_prun_structure.json\", \"w\") as json_file:\n",
    "    json.dump(prun_data, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_sst2_dynamic_qint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7886e3f-7b75-40ea-9331-f2972fb8e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prun_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19487b-6e2f-41cd-aa86-57b748fd4f2d",
   "metadata": {},
   "source": [
    "### Flash Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc2f97-32b3-48d1-9716-04a04693326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from evaluate import load\n",
    "\n",
    "model_sdpa = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-sst2\" ,attn_implementation=\"sdpa\")\n",
    "metric = load(\"glue\",\"mrpc\")\n",
    "\n",
    "device = \"cpu\"\n",
    "model_sdpa.to(device)\n",
    "\n",
    "model_sdpa.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_sdpa(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf8f24-1eb8-4f89-9904-a696fdd35d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\"\n",
    "metric = load(\"glue\",\"mrpc\")\n",
    "\n",
    "model_sdpa.to(device)\n",
    "\n",
    "model_sdpa.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_sdpa(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "# res2 = metric.compute()\n",
    "res2[\"cuda time\"] = end - start\n",
    "res2\n",
    "size_in_mb = get_model_size(model_qat)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afd9e7-720d-49cb-920c-9de7f837ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "with open(\"results/bert_sst2_sdpa.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_sst2_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_sdpa.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_sdpa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e5904-1198-43b2-bdd8-f10b56bd9a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f371559-c376-465b-b689-afcdedf6ba56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ee36d-4d84-4f2c-b06e-b85e0d2c41c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230898b-ddc9-4a67-aeee-badee2c63bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eager = BertForSequenceClassification.from_pretrained(\"pmthangk09/bert-base-uncased-glue-sst2\" ,attn_implementation=\"eager\")\n",
    "\n",
    "device = \"cpu\"\n",
    "model_eager.to(device)\n",
    "metric = load(\"glue\",\"mrpc\")\n",
    "\n",
    "model_eager.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_eager(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d13e0-5ce4-4290-93d2-9d490a3cb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\"\n",
    "model_eager.to(device)\n",
    "metric = load(\"glue\",\"mrpc\")\n",
    "\n",
    "model_eager.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        # raise error if no optimized kernel is available\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=True, enable_math=True, enable_mem_efficient=True\n",
    "        ):\n",
    "            outputs = model_eager(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "# res2 = metric.compute()\n",
    "res2[\"cuda time\"] = end - start\n",
    "res2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6cebc-1c28-4d62-9a5c-d0643fe0af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_sst2_eager.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_sst2_dynamic_qint8\")\n",
    "\n",
    "# with open(\"./models/bert_eager.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_eager, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ce3a6-a261-4b81-956d-e228cc57be69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
