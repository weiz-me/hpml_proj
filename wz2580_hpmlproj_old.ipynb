{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9742b9dc-1272-462e-b345-a1465ec8c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yes y |pip uninstall torch torchvision\n",
    "# !yes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe97718-7375-4469-96bb-1288cfb62938",
   "metadata": {},
   "source": [
    "# Fine-tune Bert MRPC\n",
    "\n",
    "Tutorial : https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb0b44b-b076-4e00-8c99-c1baae733d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020a0bc14e2844d99791fd43b313e851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/321 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe0e8e052d14e01a428b5c351ffa4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5260a70c524b2ea10fcce1c0c90b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e524bd63dd648a598c1aec977572ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7873bbc7384de183bb1cb3e9e3fc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aea65351dcc4ad9a2554ef3fe2b51db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840edf9922cb451dbf0f9347b2722fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\",\"mrpc\")\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\",\"sentence2\",\"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\",\"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e06d589-ead3-4985-ae8d-dcfa8e8441e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle = True, batch_size=8, collate_fn = data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"], batch_size=8, collate_fn = data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4b1dec-0272-47db-8755-326136f39462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "checkpoint = \"Intel/bert-base-uncased-mrpc\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e166eb-4637-42ec-89c5-62b45c769f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(),lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32991ee2-2b61-4347-ae95-128d2e103672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd19b2d-fcba-429c-a5c0-1a2c901481f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd14c7a8-0b82-4a66-912c-3db7dd1213df",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BertModel.forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     18\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: BertModel.forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "metric = load(\"glue\",config_name=\"mrpc\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)\n",
    "\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res = metric.compute()\n",
    "res[f\"{device} time\"] = end-start\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a5cbc0-adee-4c09-a158-141c10367810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import os\n",
    "metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "# res = metric.compute()\n",
    "res[f\"cpu time\"] = end-start\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb36d82-7a3b-4689-96e7-0bdb373d61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)\n",
    "\n",
    "with open(\"./models/bert_mrpc.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# with open(\"./models/bert.pkl\", \"rb\") as f:\n",
    "#     model_state_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ad108-ba53-4c00-9f65-65f5cdfe0bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40889e-a67d-445c-84b0-3fd80d4ef375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_model_size(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_size = param_size + buffer_size  # Total size in bytes\n",
    "    return total_size / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Example usage\n",
    "size_in_mb = get_model_size(model)\n",
    "res[\"size\"] = size_in_mb\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8de308-2c3d-4ed1-aab1-378f92018dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_mrpc.json\", \"w\") as json_file:\n",
    "    json.dump(res, json_file, indent=4)\n",
    "# torch.save(model, \"./models/bert_mrpc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d2571-571c-4a2d-be80-f8dbfc3589b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import get_scheduler\n",
    "\n",
    "# num_epochs = 3\n",
    "# num_training_steps = num_epochs * len(train_dataloader)\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=num_training_steps\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4bbe3-387c-4b38-af45-e6e5ef0a82e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# progress_bar = tqdm(range(num_training_steps))\n",
    "# device = \"cuda\"\n",
    "# model.to(device)\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in train_dataloader:\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092c681-5f60-4e67-9324-4f8cf7bb4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate import load\n",
    "# import time\n",
    "# import os\n",
    "# metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "# device = \"cpu\"\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# start = time.time()\n",
    "# for batch in eval_dataloader:\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**batch)\n",
    "\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "# end = time.time()\n",
    "# res = metric.compute()\n",
    "# res[f\"{device} time\"] = end-start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eaabf0-3aa3-4059-b8ba-0ad49355e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate import load\n",
    "# import time\n",
    "# import os\n",
    "# metric = load(\"glue\",config_name=\"mrpc\")\n",
    "\n",
    "# device = \"cuda\"\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# start = time.time()\n",
    "# for batch in eval_dataloader:\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**batch)\n",
    "\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "# end = time.time()\n",
    "# # res = metric.compute()\n",
    "# res[f\"{device} time\"] = end-start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22a8f2-8a9f-4c9c-9092-b64abc96785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# size_in_mb = get_model_size(model)\n",
    "# res[\"size\"] = size_in_mb\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14680664-9682-425f-ac1d-abac7bdc13b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"results/bert_mrpc_finetuned.json\", \"w\") as json_file:\n",
    "#     json.dump(res, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d434f-b8fa-4fa3-9c60-0e54ae5d0fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)\n",
    "\n",
    "# with open(\"./models/bert_mrpc_finetuned.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)\n",
    "\n",
    "# with open(\"./models/bert_mrpc_finetuned.pkl\", \"rb\") as f:\n",
    "#     model_state_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc662ca9-5b59-4bf0-ac87-885338f30a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cc93d-ac0a-46ee-9b33-bca78e382e0f",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "Tutorial: https://pytorch.org/tutorials/recipes/quantization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338a714-f281-4b81-827d-fa2da36fa2ef",
   "metadata": {},
   "source": [
    "#### dynamic quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9062752d-7031-41f3-bc3f-87c5b63cd30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./models/bert_mrpc_finetuned.pkl\", \"rb\") as f:\n",
    "#     model = pickle.load(f)\n",
    "    \n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Intel/bert-base-uncased-mrpc\",num_labels=2)\n",
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5eb7a9-506d-4403-8179-acd558c959ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "model_dynamic_quantized_int8.eval()\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_dynamic_quantized_int8(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf606cba-3231-4323-9152-75fbe0fb5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized_int8)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba54fe-980c-4246-83de-7262f14c8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_mrpc_dynamic_qint8.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n",
    "\n",
    "with open(\"./models/bert_int8.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406450a-1a45-4395-b451-caadc2ca1df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ad728-8245-4956-92ac-54ee952a5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./models/bert.pkl\", \"rb\") as f:\n",
    "#     model = pickle.load(f)\n",
    "    \n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)\n",
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424dbdf0-8513-46b8-aa7a-2ef785db2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized_int8.to(device)\n",
    "\n",
    "model_dynamic_quantized_int8.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_dynamic_quantized_int8(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cpu time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195abb19-78f0-4a4d-b769-9e8695f8d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2[\"cuda time\"] = None\n",
    "size_in_mb = get_model_size(model_dynamic_quantized)\n",
    "res2[\"size\"] = size_in_mb\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbbcfb0-b174-4e41-a8d7-242b6f871885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/bert_mrpc_dynamic_qint8.json\", \"w\") as json_file:\n",
    "    json.dump(res2, json_file, indent=4)\n",
    "# torch.save(model_dynamic_quantized, \"./models/bert_mrpc_dynamic_qint8\")\n",
    "\n",
    "with open(\"./models/bert_int8.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a1efa0-1058-4f93-b99d-68ce60cac488",
   "metadata": {},
   "source": [
    "#### Quantization Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3590bf-85d3-4f95-939a-1d2920a491a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### code for static quantized\n",
    "# backend = \"qnnpack\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)\n",
    "# model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "# torch.backends.quantized.engine = backend\n",
    "# model_static_quantized = torch.quantization.prepare(model, inplace=False)\n",
    "# model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a7f8f4-36de-4b31-885c-1f2334416032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomQuantizedBert(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", num_labels=2):\n",
    "        super(CustomQuantizedBert, self).__init__()\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)\n",
    "\n",
    "        # Quantize embeddings\n",
    "        self.quant_embeddings = torch.ao.quantization.QuantStub()\n",
    "        self.dequant_embeddings = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "        # Quantize classifier\n",
    "        self.quant_classifier = torch.ao.quantization.QuantStub()\n",
    "        self.dequant_classifier = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        # Quantize inputs for embeddings\n",
    "        input_ids = self.quant_embeddings(input_ids)\n",
    "        \n",
    "        # Forward pass through BERT\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        \n",
    "        # Quantize classifier output\n",
    "        logits = self.quant_classifier(outputs.logits)\n",
    "        logits = self.dequant_classifier(logits)\n",
    "        return logits,outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b6a01-1844-4845-8010-88f93b52f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qat = CustomQuantizedBert()\n",
    "device = \"cuda\"\n",
    "model_qat.to(device)\n",
    "\n",
    "model_qat.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs,loss = model_qat(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cuda time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc928b4-f0b4-446b-88cc-882cc7fbae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b77641-2c8a-4370-8420-319c893e7875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model_qat.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs,loss = model_qat(**batch)\n",
    "        # loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9184cf-5c44-4253-b80a-a135e0cded97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qat = CustomQuantizedBert()\n",
    "device = \"cuda\"\n",
    "model_qat.to(device)\n",
    "\n",
    "model_qat.eval()\n",
    "start = time.time()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs,loss = model_qat(**batch)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "    logits = outputs\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "end = time.time()\n",
    "res2 = metric.compute()\n",
    "res2[\"cuda time\"] = end - start\n",
    "res2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f460d31-cf6c-4145-bfed-07d62df10178",
   "metadata": {},
   "source": [
    "#### Quantization Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fd3482-2849-44e9-95ed-ce330b7b877e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20c7a1-3107-446e-8068-ae5325c2e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model_dynamic_quantized = torch.quantization.quantize_dynamic(\n",
    "    model, dtype=torch.qint8\n",
    ")\n",
    "model_dynamic_quantized.to(device)\n",
    "\n",
    "model_dynamic_quantized.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_dynamic_quantized(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ee36d-4d84-4f2c-b06e-b85e0d2c41c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
